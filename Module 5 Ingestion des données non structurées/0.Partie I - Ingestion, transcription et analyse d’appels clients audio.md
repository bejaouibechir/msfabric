# Atelier 5 : Ingestion, transcription et enrichissement IA d’appels clients audio (énergies renouvelables) dans Microsoft Fabric

**Objectif concret**  
Vous allez charger 120 fichiers audio MP3 d’appels clients (thème panneaux solaires / PAC / bornes VE), les transcrire en texte, analyser le sentiment et l’intent, puis créer une table Delta exploitable en Direct Lake dans Power BI.

**Résultat final visible**

- Table Delta `silver.appels_clients_enrichis`
- Rapport Power BI avec transcription, sentiment, intent et lien vers l’audio original
- Dashboard filtrable par niveau de négativité / type de demande

## Fichiers nécessaires (à préparer avant l’atelier)

- Dossier `mock_appels_mp3/` contenant exactement 120 fichiers : `appel_001.mp3` … `appel_120.mp3`  
  (générés avec le script `generate_voices.py` que vous avez déjà)
- Fichier `metadata.csv` (facultatif mais très utile pour croisement) – contenu fourni en fin d’atelier

## Étape 1 – Création de l’environnement Fabric

1. Ouvrez **Microsoft Fabric** → **Espaces de travail**
2. Cliquez **+ Nouvel espace de travail**
3. Nom : `WS_VoixClients_Energetique`
4. Dans cet espace → **+ Nouveau** → **Lakehouse**
5. Nom du Lakehouse : `LH_AppelsClients_2026`
6. Attendez que le Lakehouse soit créé (icône verte)

## Étape 2 – Upload des fichiers audio

1. Dans le Lakehouse `LH_AppelsClients_2026` → onglet **Files**
2. Cliquez **Nouveau dossier** → nom : `audio_raw`
3. Ouvrez le dossier `audio_raw`
4. Cliquez **Charger** → **Dossier** → sélectionnez votre dossier local `mock_appels_mp3`
5. Attendez la fin du transfert (120 fichiers)

**Vérification rapide**  
Dans la zone Files :  
→ chemin attendu : `Files/audio_raw/mock_appels_mp3/appel_001.mp3` … `appel_120.mp3`

## Étape 3 – Création et configuration du Notebook

1. Dans le Lakehouse → **+ Nouveau** → **Notebook**
2. Renommez-le : `NB_01_Ingestion_Transcription_Enrichissement`
3. Attachez-le à la **capacité Fabric** par défaut de votre workspace

Exécutez les cellules suivantes dans l’ordre :

**Cellule 1 – Imports et configuration**

```python
from pyspark.sql import functions as F
from pyspark.sql.types import *
from synapse.ml.cognitive import *
from synapse.ml.core.platform import *

print("Imports terminés")

cognitive_key = "VOTRE_CLE_AZURE_AI_SERVICES" # ← remplacez
cognitive_region = "francecentral" # ou westeurope

print("Clé et région configurées")
```

**Cellule 2 – Lecture des fichiers binaires**

```python
df_audio = spark.read.format("binaryFile") \
    .option("recursiveFileLookup", "true") \
    .option("pathGlobFilter", "*.mp3") \
    .load("Files/audio_raw/mock_appels_mp3/")

print("Nombre de fichiers chargés :", df_audio.count())

display(df_audio.select(
    F.element_at(F.split("path", "/"), -1).alias("filename"),
    "length", "modificationTime"
).limit(10))
```

**Cellule 3 – Transcription Speech-to-Text**

```python
speech = (SpeechToTextSDK()
    .setSubscriptionKey(cognitive_key)
    .setLocation(cognitive_region)
    .setLanguage("fr-FR")
    .setOutputFormat("detailed")
    .setProfanity("masked")
    .setConcurrency(12)  # adaptez selon votre capacity
)

df_transcrit = speech.transform(df_audio)

df_transcrit = df_transcrit.withColumn(
    "filename",
    F.element_at(F.split("path", "/"), -1)
).withColumnRenamed("displayText", "transcription") \
 .withColumnRenamed("confidence", "transcription_confidence")

display(df_transcrit.select(
    "filename", "transcription", "transcription_confidence"
).limit(8))
```

**Cellule 4 – Analyse de sentiment**

```python
sentiment = (TextSentiment()
    .setSubscriptionKey(cognitive_key)
    .setLocation(cognitive_region)
    .setTextCol("transcription")
    .setOutputCol("sentiment_details")
)

df_sentiment = sentiment.transform(df_transcrit)

df_sentiment = df_sentiment.select(
    "*",
    F.col("sentiment_details")[0]["sentiment"].alias("sentiment"),
    F.col("sentiment_details")[0]["confidenceScores"]["positive"].alias("score_pos"),
    F.col("sentiment_details")[0]["confidenceScores"]["negative"].alias("score_neg"),
    F.col("sentiment_details")[0]["confidenceScores"]["neutral"].alias("score_neu")
)

df_sentiment = df_sentiment.withColumn(
    "niveau_sentiment",
    F.when(F.col("score_neg") >= 0.70, "Très négatif")
     .when(F.col("score_neg") >= 0.50, "Négatif")
     .when(F.col("score_pos") >= 0.70, "Très positif")
     .when(F.col("score_pos") >= 0.50, "Positif")
     .otherwise("Neutre / Mixte")
)

display(df_sentiment.select(
    "filename", "transcription", "sentiment", "niveau_sentiment",
    F.round("score_neg", 3).alias("négatif"), F.round("score_pos", 3).alias("positif")
).orderBy(F.desc("score_neg")).limit(10))
```

**Cellule 5 – Détection d’intent (règles + fallback)**

```python
df_intent = df_sentiment.withColumn(
    "intent",
    F.when(F.lower("transcription").contains("résili") | F.lower("transcription").contains("annuler"), "résiliation")
     .when(F.lower("transcription").contains("facture") | F.lower("transcription").contains("prime") | F.lower("transcription").contains("maPrimeRénov"), "facturation / aides")
     .when(F.lower("transcription").contains("panne") | F.lower("transcription").contains("onduleur") | F.lower("transcription").contains("erreur") | F.lower("transcription").contains("ne marche"), "support technique")
     .when(F.lower("transcription").contains("merci") | F.lower("transcription").contains("satisfait") | F.lower("transcription").contains("bravo") | F.lower("transcription").contains("super"), "compliment / satisfaction")
     .when(F.lower("transcription").contains("comment") | F.lower("transcription").contains("quelle") | F.lower("transcription").contains("aides") | F.lower("transcription").contains("éligible"), "demande d’information")
     .when(F.lower("transcription").contains("toit") | F.lower("transcription").contains("infiltration") | F.lower("transcription").contains("pose") | F.lower("transcription").contains("installat"), "problème installation / SAV")
     .otherwise("autre")
)

display(df_intent.groupBy("intent").count().orderBy(F.desc("count")))
```

**Cellule 6 – Sauvegarde finale en Delta Table**

```python
df_final = df_intent.select(
    "filename", "path", "transcription", "transcription_confidence",
    "sentiment", "niveau_sentiment", "score_pos", "score_neg", "score_neu",
    "intent"
)

df_final.write \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .format("delta") \
    .saveAsTable("silver.appels_clients_energetique")

print("Table créée : silver.appels_clients_energetique")
```

**Cellule 7 – Contrôles qualité finaux**

```python
%sql
SELECT 
    niveau_sentiment,
    COUNT(*) AS volume,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) AS pct
FROM silver.appels_clients_energetique
GROUP BY niveau_sentiment
ORDER BY pct DESC
```

```sql
SELECT intent, COUNT(*) AS volume
FROM silver.appels_clients_energetique
GROUP BY intent
ORDER BY volume DESC
LIMIT 8
```

## Étape finale – Rapport Power BI Direct Lake

1. Dans le Lakehouse → onglet **Tables** → clic droit sur `silver.appels_clients_energetique` → **Nouveau rapport Power BI**
2. Ajoutez un visuel **Tableau** avec :
   - filename
   - transcription (format texte multiligne)
   - niveau_sentiment (couleurs conditionnelles : rouge pour Très négatif)
   - intent
   - score_neg (jauge ou carte thermique)
3. Ajoutez un **Slicer** sur `niveau_sentiment` et `intent`

**Astuce** : Activez **Direct Lake** pour zéro latence sur les 120 lignes.

**Piège courant** : Si le rapport est vide → vérifiez que la table est bien en mode **Direct Lake** (pas Import).

---

**Prochain atelier** : Atelier 6 – Pipeline Data Factory déclenché par arrivée de fichier + alerte Activator sur appels très négatifs

Voulez-vous que je vous prépare directement l’atelier 6 maintenant ?

```

```
