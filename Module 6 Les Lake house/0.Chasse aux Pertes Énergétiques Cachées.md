# √âtude de Cas n¬∞1 ‚Äì Parc √âolien : Chasse aux Pertes √ânerg√©tiques Cach√©es

---

## √ânonc√©

### 1. √âtat des lieux : Le paradoxe du parc √©olien

Imaginez-vous face √† un tableau de bord √©nerg√©tique qui ne raconte pas toute l'histoire. Votre client, √âoliennes France, exploite 50 g√©ants des vents r√©partis sur trois r√©gions. En surface, tout semble normal : les pales tournent, les compteurs additionnent les m√©gawattheures, les rapports mensuels sont produits.

Mais un chiffre clignote en rouge : **32% de facteur de charge global** contre **38% attendu**.

Traduit en langage financier : **1,2 million d'euros** de production qui s'√©vaporent chaque ann√©e, comme du sable entre les doigts. Les ing√©nieurs sur le terrain soup√ßonnent des probl√®mes, mais leurs outils sont limit√©s √† des feuilles Excel qui plantent √† 100 000 lignes, alors que vos donn√©es en comptent **15 millions**.

Vous avez en main :

- 50 turbines, chacune avec sa propre personnalit√©, son historique, ses caprices ;
- Des donn√©es de production toutes les 15 minutes ;
- Des mesures m√©t√©o qui dansent avec le vent ;
- Un calendrier de maintenance qui ne raconte pas toujours toute la v√©rit√©.

Le constat est amer : vous naviguez √† l'aveugle dans un oc√©an de donn√©es.

### 2. Probl√©matique : L'aiguille dans la botte de foin √©nerg√©tique

Le directeur technique vous fixe : *¬´ Certaines turbines nous trahissent. Elles tournent mais ne produisent pas. Lequel de mes 50 enfants prodiges gaspille ma ressource ? ¬ª*

Les sympt√¥mes sont clairs, mais le diagnostic reste flou :

- **Huit turbines** montrent des courbes de production √©trangement plates ;
- Les rapports de maintenance ne mentionnent aucun probl√®me ;
- Les techniciens sur place ne voient rien d'anormal ;
- Pourtant, les chiffres ne mentent pas : le parc sous-performe de **6 %**.

Votre mission : identifier pr√©cis√©ment quelles turbines produisent **moins de 60 % de leur potentiel th√©orique** et pourquoi.

Quatre hypoth√®ses s'affrontent :

1. **Hypoth√®se technique** : certaines turbines vieillissent mal, leurs courbes de puissance s'√©tiolent ;
2. **Hypoth√®se op√©rationnelle** : des arr√™ts non document√©s grignotent la production ;
3. **Hypoth√®se environnementale** : les turbines se volent mutuellement le vent ;
4. **Hypoth√®se syst√©mique** : c'est un probl√®me de mesure, pas de production.

Le temps presse : chaque jour perdu repr√©sente **3 300 ‚Ç¨** √©vapor√©s.

### 3. Solution attendue : L'autopsie num√©rique du parc

C'est ici que l'architecture **Medallion (Bronze / Silver / Gold)** du Lakehouse entre en sc√®ne, coupl√©e √† la double motorisation **Spark SQL** et **PySpark**.

#### Phase 1 ‚Äì L'exploration (Bronze ‚Äì Silver simple)

En quelques requ√™tes Spark SQL, vous ouvrez les donn√©es brutes. Un pattern √©merge imm√©diatement : **8 turbines** voient leur production s'effondrer aux heures de vent optimal.
Vous croisez production et maintenance : **5 des 8 arr√™ts suspects ne sont pas couverts par la maintenance d√©clar√©e**. Premier point marqu√©.

#### Phase 2 ‚Äì L'investigation approfondie (Silver ‚Äì calculs avanc√©s)

Maintenant, la question cruciale : quand ces turbines tournent, produisent-elles ce qu'elles devraient ?
La courbe de puissance th√©orique du constructeur est une fonction non lin√©aire, impossible √† mod√©liser en SQL pur. Vous passez en **PySpark** et d√©finissez une **UDF** qui, pour chaque vitesse de vent, calcule la production attendue.
Le calcul du **ratio performance** (r√©el / th√©orique) tombe : **3 turbines affichent un ratio < 60 %** de fa√ßon syst√©matique. Le coupable se pr√©cise : des **pales endommag√©es**, invisibles depuis le sol, flagrantes dans les donn√©es.

Pour aller plus loin, vous entra√Ænez un mod√®le de r√©gression (PySpark MLlib) qui confirme les d√©rives et d√©tecte m√™me une **quatri√®me turbine** avec une d√©gradation naissante.

#### Phase 3 ‚Äì La synth√®se actionnable (Gold ‚Äì visualisation)

Les donn√©es sont agr√©g√©es au niveau quotidien et par turbine, les KPIs m√©tier sont pr√™ts. Le tableau de bord final r√©v√®le :

- **3 inspections de pales** (co√ªt 15 000 ‚Ç¨, gain 180 000 ‚Ç¨/an) ;
- **5 arr√™ts non document√©s** √† investiguer (probl√®me process ou capteur) ;
- **1 effet d'ombrage** entre deux turbines trop proches.

**Gain total estim√© : 450 000 ‚Ç¨ de production r√©cup√©rable d√®s la premi√®re ann√©e.**

---

## Mise en ≈ìuvre ‚Äî Atelier pratique

Vous allez construire la solution compl√®te dans un Lakehouse Fabric : g√©n√©rer 15 millions de lignes de donn√©es simul√©es, les ing√©rer en Bronze, les transformer en Silver avec Spark SQL puis PySpark (UDF + MLlib), et produire les tables Gold pour le tableau de bord.

---

## Fichiers n√©cessaires

Tous les fichiers sont g√©n√©r√©s par les scripts ci-dessous. Aucun t√©l√©chargement externe.

---

## Partie 1 ‚Äî G√©n√©rer les donn√©es simul√©es

### 1.1 ‚Äî Script Python : g√©n√©rateur du jeu de donn√©es

Cr√©er **`generate_wind_farm_data.py`** sur votre poste local :

```python
import csv
import random
import os
from datetime import datetime, timedelta

random.seed(42)

# ============================================================
# CONFIGURATION
# ============================================================
OUTPUT_DIR = r"D:\msfabric\windfarm_data"
NB_TURBINES = 50
YEAR = 2025
INTERVAL_MINUTES = 15  # une mesure toutes les 15 min
# ============================================================

os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Referentiel turbines ---
REGIONS = {
    "Hauts-de-France": list(range(1, 21)),       # 20 turbines
    "Grand-Est": list(range(21, 41)),             # 20 turbines
    "Occitanie": list(range(41, 51)),             # 10 turbines
}

TURBINE_MODELS = {
    "Vestas V110": {"rated_power_kw": 2200, "cut_in": 3, "rated_speed": 11.5, "cut_out": 25},
    "Siemens SWT-3.0": {"rated_power_kw": 3000, "cut_in": 3.5, "rated_speed": 12, "cut_out": 25},
    "Enercon E-115": {"rated_power_kw": 3000, "cut_in": 2.5, "rated_speed": 12.5, "cut_out": 25},
}

# Affecter un modele a chaque turbine
turbine_info = {}
for region, ids in REGIONS.items():
    for tid in ids:
        model_name = random.choice(list(TURBINE_MODELS.keys()))
        specs = TURBINE_MODELS[model_name]
        turbine_info[tid] = {
            "turbine_id": f"WT-{tid:03d}",
            "region": region,
            "model": model_name,
            "rated_power_kw": specs["rated_power_kw"],
            "cut_in": specs["cut_in"],
            "rated_speed": specs["rated_speed"],
            "cut_out": specs["cut_out"],
            "install_year": random.randint(2012, 2022),
            "latitude": round(random.uniform(42.5, 50.8), 4),
            "longitude": round(random.uniform(-1.0, 7.5), 4),
        }

# Les 8 turbines problematiques (selon l'enonce)
DEGRADED_TURBINES = [3, 7, 12, 19, 25, 33, 38, 47]
# 3 avec pales endommagees (ratio < 60%), 2 avec arrets non documentes,
# 2 avec degradation naissante, 1 avec effet d'ombrage
BLADE_DAMAGE = [7, 25, 38]          # ratio < 60%
UNDOCUMENTED_STOPS = [3, 12, 19, 33, 47]  # 5 arrets non documentes
NASCENT_DEGRADATION = [19, 47]       # degradation naissante (parmi les stops)
WAKE_EFFECT = [33]                   # effet d'ombrage


def theoretical_power(wind_speed, specs):
    """Courbe de puissance theorique (approximation cubique)."""
    if wind_speed < specs["cut_in"] or wind_speed > specs["cut_out"]:
        return 0
    if wind_speed >= specs["rated_speed"]:
        return specs["rated_power_kw"]
    ratio = (wind_speed - specs["cut_in"]) / (specs["rated_speed"] - specs["cut_in"])
    return specs["rated_power_kw"] * (ratio ** 3)


def generate_wind_speed(hour, month, region):
    """Vitesse du vent simulee (m/s) avec saisonnalite et cycle journalier."""
    # Base saisonniere
    seasonal = 7 + 3 * ((month in [11, 12, 1, 2, 3]) - (month in [6, 7, 8]) * 0.5)
    # Effet region
    region_bonus = {"Hauts-de-France": 1.5, "Grand-Est": 0.5, "Occitanie": -0.5}
    base = seasonal + region_bonus.get(region, 0)
    # Cycle journalier (vent plus fort l'apres-midi)
    daily = 1.0 + 0.3 * (1 if 12 <= hour <= 18 else -0.2)
    speed = base * daily + random.gauss(0, 2.5)
    return max(0, round(speed, 1))


# --- Ecriture du referentiel turbines ---
ref_path = os.path.join(OUTPUT_DIR, "turbine_reference.csv")
with open(ref_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["turbine_id", "region", "model", "rated_power_kw",
                "cut_in_ms", "rated_speed_ms", "cut_out_ms",
                "install_year", "latitude", "longitude"])
    for tid, info in turbine_info.items():
        w.writerow([info["turbine_id"], info["region"], info["model"],
                    info["rated_power_kw"], info["cut_in"],
                    info["rated_speed"], info["cut_out"],
                    info["install_year"], info["latitude"], info["longitude"]])
print(f"Referentiel: {ref_path} ({NB_TURBINES} turbines)")


# --- Maintenance planifiee ---
maint_path = os.path.join(OUTPUT_DIR, "maintenance_log.csv")
maint_records = []
with open(maint_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["turbine_id", "maintenance_start", "maintenance_end",
                "maintenance_type", "description"])
    for tid in range(1, NB_TURBINES + 1):
        # 3-5 maintenances par turbine par an
        for _ in range(random.randint(3, 5)):
            start_day = random.randint(1, 350)
            duration_h = random.choice([4, 8, 12, 24, 48])
            start_dt = datetime(YEAR, 1, 1) + timedelta(days=start_day, hours=random.randint(6, 14))
            end_dt = start_dt + timedelta(hours=duration_h)
            m_type = random.choice(["preventive", "corrective", "inspection"])
            desc = random.choice([
                "Inspection visuelle standard",
                "Remplacement filtre huile",
                "Verification pitch system",
                "Graissage paliers",
                "Calibration anemoetre",
                "Remplacement joint generateur",
                "Inspection pale par drone",
            ])
            w.writerow([f"WT-{tid:03d}", start_dt.isoformat(), end_dt.isoformat(), m_type, desc])
            maint_records.append((tid, start_dt, end_dt))
print(f"Maintenance: {maint_path}")


# --- Donnees de production (le gros fichier) ---
prod_path = os.path.join(OUTPUT_DIR, "production_data.csv")
total_rows = 0
with open(prod_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["timestamp", "turbine_id", "wind_speed_ms", "wind_direction_deg",
                "ambient_temp_c", "rotor_speed_rpm", "power_output_kw",
                "nacelle_direction_deg", "pitch_angle_deg", "status"])

    start_dt = datetime(YEAR, 1, 1, 0, 0)
    end_dt = datetime(YEAR, 12, 31, 23, 45)
    current = start_dt

    while current <= end_dt:
        month = current.month
        hour = current.hour

        for tid in range(1, NB_TURBINES + 1):
            info = turbine_info[tid]
            specs = TURBINE_MODELS[info["model"]]

            wind = generate_wind_speed(hour, month, info["region"])
            wind_dir = random.uniform(0, 360)
            temp = 10 + 10 * ((month - 1) / 11) + random.gauss(0, 3)

            # Verifier maintenance planifiee
            in_maintenance = any(
                t == tid and s <= current <= e for t, s, e in maint_records
            )

            if in_maintenance:
                status = "maintenance"
                power = 0
                rotor = 0
                pitch = 90
            elif wind < specs["cut_in"] or wind > specs["cut_out"]:
                status = "standby"
                power = 0
                rotor = 0
                pitch = 90 if wind > specs["cut_out"] else 0
            else:
                status = "running"
                theo_power = theoretical_power(wind, specs)
                # Rendement normal ~92-98%
                efficiency = random.uniform(0.92, 0.98)

                # --- INJECTION DES ANOMALIES ---
                if tid in BLADE_DAMAGE:
                    # Pales endommagees : ratio < 60% systematique
                    efficiency = random.uniform(0.35, 0.58)

                elif tid in NASCENT_DEGRADATION:
                    # Degradation naissante : ratio ~70-80%
                    efficiency = random.uniform(0.65, 0.80)

                elif tid in WAKE_EFFECT and 180 <= wind_dir <= 270:
                    # Effet d'ombrage quand vent de SO
                    efficiency = random.uniform(0.50, 0.70)

                # Arrets non documentes (5 turbines, ~2% du temps chacune)
                if tid in UNDOCUMENTED_STOPS and random.random() < 0.02:
                    status = "running"  # Le status dit running mais power = 0
                    power = random.uniform(0, 5)  # quasi-zero
                    rotor = random.uniform(0, 1)
                    pitch = random.uniform(0, 5)
                else:
                    power = theo_power * efficiency + random.gauss(0, 20)
                    power = max(0, round(power, 2))
                    rotor = 8 + (wind / specs["rated_speed"]) * 7 + random.gauss(0, 0.5)
                    pitch = max(0, min(30, (wind - specs["rated_speed"]) * 3 + random.gauss(0, 1)))

            w.writerow([
                current.isoformat(),
                f"WT-{tid:03d}",
                wind,
                round(wind_dir, 1),
                round(temp, 1),
                round(max(0, rotor), 2),
                round(max(0, power), 2),
                round(wind_dir + random.gauss(0, 5), 1),
                round(max(0, pitch), 2),
                status
            ])
            total_rows += 1

        current += timedelta(minutes=INTERVAL_MINUTES)

        if total_rows % 500_000 == 0:
            print(f"  {total_rows:,} lignes generees...")

print(f"\nProduction: {prod_path}")
print(f"Total: {total_rows:,} lignes")
print(f"\nTurbines problematiques injectees:")
print(f"  Pales endommagees (ratio < 60%): WT-007, WT-025, WT-038")
print(f"  Arrets non documentes: WT-003, WT-012, WT-019, WT-033, WT-047")
print(f"  Degradation naissante: WT-019, WT-047")
print(f"  Effet d'ombrage: WT-033")
```

### 1.2 ‚Äî Lancer la g√©n√©ration

```bash
py generate_wind_farm_data.py
```

**Dur√©e** : 5-15 minutes selon votre machine. **Fichiers produits** :

| Fichier                 | Taille estim√©e | Lignes     |
| ----------------------- | -------------- | ---------- |
| `production_data.csv`   | ~1.5 Go        | ~1 750 000 |
| `turbine_reference.csv` | ~3 Ko          | 50         |
| `maintenance_log.csv`   | ~15 Ko         | ~200       |

> ‚ö†Ô∏è **Pi√®ge** : Le fichier production fait ~1.5 Go. L'upload vers le Lakehouse sera plus simple par morceaux ou via OneLake File Explorer (voir √©tape 2.3).

> üí° **Astuce** : Pour un test rapide, r√©duisez la p√©riode. Changez `end_dt = datetime(YEAR, 3, 31, 23, 45)` pour ne g√©n√©rer que le Q1 (~440 000 lignes).

---

## Partie 2 ‚Äî Lakehouse : Ingestion Bronze

### 2.1 ‚Äî Cr√©er le Lakehouse

1. **app.fabric.microsoft.com** ‚Üí votre workspace
2. **+ New item** ‚Üí **Lakehouse**
3. Nom : **`LH_WindFarm`**
4. Cliquer **Create**

### 2.2 ‚Äî Cr√©er la structure de dossiers

1. Dans **LH_WindFarm** ‚Üí **Files** ‚Üí clic droit ‚Üí **New subfolder** ‚Üí **`bronze`**
2. Dans **bronze** ‚Üí **New subfolder** ‚Üí **`production`**
3. Dans **bronze** ‚Üí **New subfolder** ‚Üí **`reference`**
4. Dans **bronze** ‚Üí **New subfolder** ‚Üí **`maintenance`**

### 2.3 ‚Äî Charger les fichiers

**Option A ‚Äî Upload direct (fichiers < 1 Go)** :

1. Cliquer sur le dossier **bronze/reference** ‚Üí **Upload** ‚Üí **Upload files** ‚Üí s√©lectionner `turbine_reference.csv`
2. Idem pour **bronze/maintenance** ‚Üí `maintenance_log.csv`
3. Pour **bronze/production** ‚Üí `production_data.csv`

**Option B ‚Äî OneLake File Explorer (recommand√© pour les gros fichiers)** :

1. Installer **OneLake File Explorer** depuis le Microsoft Store
2. Naviguer vers `OneLake / <workspace> / LH_WindFarm.Lakehouse / Files / bronze / production`
3. Glisser-d√©poser `production_data.csv`

> üí° **Astuce** : Pour un fichier de 1.5 Go, l'upload navigateur peut √©chouer. OneLake File Explorer est fiable m√™me pour des fichiers de plusieurs Go.

---

## Partie 3 ‚Äî Phase 1 : Exploration (Bronze ‚Üí Silver simple) avec Spark SQL

> üí° **Runtime requis** : Cet atelier fonctionne avec **Runtime 1.3** (Spark 3.5, Delta Lake 3.2, Python 3.11) ‚Äî le runtime GA par d√©faut pour les nouveaux workspaces. V√©rifiez dans **Workspace Settings** ‚Üí **Data Engineering/Science** ‚Üí **Spark settings** ‚Üí **Runtime version** : **1.3**.

> üí° **Performance gratuite** : Activez le **Native Execution Engine** (jusqu'√† 4x plus rapide, sans surco√ªt). Dans votre workspace ‚Üí **Manage** ‚Üí **Spark settings** ‚Üí onglet **Acceleration** ‚Üí cocher ‚úÖ **Enable native execution engine** ‚Üí **Save**.

### 3.1 ‚Äî Cr√©er le Notebook

1. Workspace ‚Üí **+ New item** ‚Üí **Notebook**
2. Nommer : **`NB_WindFarm_Analysis`**
3. Attacher au Lakehouse **LH_WindFarm** (s√©lecteur en haut √† gauche)

### 3.2 ‚Äî Cellule 1 : Chargement et cr√©ation des tables Bronze

**Probl√®me terrain** : Les donn√©es sont dans des CSV bruts. Il faut les rendre requ√™tables.

```python
# === CELLULE 1 : Chargement Bronze ===

# Production (le gros fichier)
df_prod = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/production/production_data.csv")
)
df_prod.write.mode("overwrite").format("delta").saveAsTable("bronze_production")

# Reference turbines
df_ref = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/reference/turbine_reference.csv")
)
df_ref.write.mode("overwrite").format("delta").saveAsTable("bronze_turbine_ref")

# Maintenance
df_maint = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/maintenance/maintenance_log.csv")
)
df_maint.write.mode("overwrite").format("delta").saveAsTable("bronze_maintenance")

print(f"Production : {df_prod.count():,} lignes")
print(f"Reference  : {df_ref.count()} turbines")
print(f"Maintenance: {df_maint.count()} interventions")
```

**Interpr√©tation** : Les CSV bruts sont maintenant des tables Delta Lake, index√©es et partitionn√©es. Le format Delta avec **V-Order** (activ√© par d√©faut dans Fabric) optimise automatiquement la compression et les performances de lecture. Les requ√™tes Spark SQL peuvent commencer.

### 3.3 ‚Äî Cellule 2 : Vue d'ensemble du parc (Spark SQL)

**Probl√®me terrain** : Le directeur technique veut un premier panorama avant de plonger dans les d√©tails.

**Question m√©tier** : Quel est le facteur de charge global et par r√©gion ?

> ‚ö†Ô∏è **Pi√®ge cellule SQL** : Dans un Notebook Fabric, pour ex√©cuter du **Spark SQL** dans une cellule, ajoutez `%%sql` en premi√®re ligne de la cellule. Sans cette directive, le notebook attend du PySpark.

```sql
%%sql
-- === CELLULE 2 (SQL) : Facteur de charge par region ===

SELECT
    r.region,
    COUNT(DISTINCT p.turbine_id) AS nb_turbines,
    ROUND(AVG(p.power_output_kw), 1) AS avg_power_kw,
    ROUND(AVG(r.rated_power_kw), 0) AS rated_power_kw,
    ROUND(AVG(p.power_output_kw) / AVG(r.rated_power_kw) * 100, 1) AS capacity_factor_pct,
    ROUND(SUM(p.power_output_kw * 0.25) / 1000, 0) AS total_production_mwh
FROM bronze_production p
JOIN bronze_turbine_ref r ON p.turbine_id = r.turbine_id
WHERE p.status = 'running'
GROUP BY r.region
ORDER BY capacity_factor_pct DESC
```

**Interpr√©tation** : On voit imm√©diatement si une r√©gion enti√®re sous-performe ou si le probl√®me est localis√©. Les Hauts-de-France devraient afficher un meilleur facteur de charge (plus de vent) que l'Occitanie. Si ce n'est pas le cas, le probl√®me est dans les turbines, pas dans le vent.

### 3.4 ‚Äî Cellule 3 : Identifier les turbines suspectes

**Probl√®me terrain** : Huit turbines montrent des courbes plates. Lesquelles exactement ?

**Question m√©tier** : Quelles turbines produisent moins de 60% de leur potentiel ?

```sql
%%sql
-- === CELLULE 3 (SQL) : Classement des turbines par performance ===

SELECT
    p.turbine_id,
    r.region,
    r.model,
    r.rated_power_kw,
    COUNT(*) AS nb_readings,
    ROUND(AVG(p.power_output_kw), 1) AS avg_power_kw,
    ROUND(AVG(p.power_output_kw) / r.rated_power_kw * 100, 1) AS avg_ratio_pct,
    ROUND(AVG(p.wind_speed_ms), 1) AS avg_wind_ms,
    SUM(CASE WHEN p.power_output_kw < 5 AND p.status = 'running' THEN 1 ELSE 0 END) AS ghost_running_count
FROM bronze_production p
JOIN bronze_turbine_ref r ON p.turbine_id = r.turbine_id
WHERE p.status = 'running'
GROUP BY p.turbine_id, r.region, r.model, r.rated_power_kw
ORDER BY avg_ratio_pct ASC
```

**Interpr√©tation** : Les turbines en bas du classement sont les suspectes. La colonne `ghost_running_count` r√©v√®le les cas o√π la turbine d√©clare tourner mais produit quasi-rien ‚Äî signature typique d'un arr√™t non document√© ou d'un capteur d√©faillant. Si `avg_wind_ms` est normal mais `avg_ratio_pct` est bas ‚Üí le probl√®me est dans la turbine, pas dans le vent.

### 3.5 ‚Äî Cellule 4 : Croiser production et maintenance

**Probl√®me terrain** : Cinq des huit turbines suspectes n'ont aucune maintenance d√©clar√©e qui explique leurs arr√™ts.

**Question m√©tier** : Les baisses de production sont-elles couvertes par des interventions planifi√©es ?

> üí° **Astuce** : La cellule 4 contient un `CREATE TEMP VIEW` puis une `SELECT`. Dans Fabric, vous pouvez mettre les deux dans la **m√™me cellule SQL** (s√©par√©es par `;`) ou dans deux cellules `%%sql` s√©par√©es.

```sql
%%sql
-- === CELLULE 4 (SQL) : Periodes de production nulle sans maintenance ===

CREATE OR REPLACE TEMP VIEW v_zero_production AS
SELECT
    p.turbine_id,
    p.timestamp,
    p.power_output_kw,
    p.wind_speed_ms,
    p.status
FROM bronze_production p
WHERE p.status = 'running'
  AND p.power_output_kw < 5
  AND p.wind_speed_ms > 4;

SELECT
    z.turbine_id,
    COUNT(*) AS zero_prod_readings,
    SUM(CASE WHEN m.turbine_id IS NULL THEN 1 ELSE 0 END) AS unexplained_count,
    ROUND(SUM(CASE WHEN m.turbine_id IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) AS unexplained_pct
FROM v_zero_production z
LEFT JOIN bronze_maintenance m
    ON z.turbine_id = m.turbine_id
    AND z.timestamp BETWEEN m.maintenance_start AND m.maintenance_end
GROUP BY z.turbine_id
HAVING COUNT(*) > 10
ORDER BY unexplained_count DESC
```

**Interpr√©tation** : Un `unexplained_pct` de 90%+ signifie que la turbine s'arr√™te souvent sans raison d√©clar√©e. Soit le processus de d√©claration de maintenance est d√©faillant (probl√®me organisationnel), soit la turbine a un d√©faut intermittent non d√©tect√© (probl√®me technique). Dans les deux cas, l'exploitant perd de l'argent sans le savoir.

---

## Partie 4 ‚Äî Phase 2 : Investigation approfondie (Silver avanc√©) avec PySpark

### 4.1 ‚Äî Cellule 5 : UDF ‚Äî Courbe de puissance th√©orique

**Probl√®me terrain** : La courbe de puissance du constructeur est non lin√©aire (cubique). SQL ne peut pas la mod√©liser.

**Question m√©tier** : Pour chaque mesure, quelle est la production th√©orique attendue par le constructeur ?

```python
# === CELLULE 5 : UDF courbe de puissance theorique ===

from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType

# Charger le referentiel pour avoir les specs par turbine
df_ref = spark.table("bronze_turbine_ref")
ref_map = {row.turbine_id: row.asDict() for row in df_ref.collect()}

# Broadcast du referentiel (petit dataset)
ref_broadcast = spark.sparkContext.broadcast(ref_map)

@F.udf(DoubleType())
def calc_theoretical_power(turbine_id, wind_speed):
    """Calcule la puissance theorique selon la courbe constructeur."""
    specs = ref_broadcast.value.get(turbine_id)
    if specs is None or wind_speed is None:
        return 0.0

    cut_in = float(specs["cut_in_ms"])
    rated_speed = float(specs["rated_speed_ms"])
    cut_out = float(specs["cut_out_ms"])
    rated_power = float(specs["rated_power_kw"])

    if wind_speed < cut_in or wind_speed > cut_out:
        return 0.0
    if wind_speed >= rated_speed:
        return rated_power

    ratio = (wind_speed - cut_in) / (rated_speed - cut_in)
    return rated_power * (ratio ** 3)


# Appliquer l'UDF sur les donnees de production
df_prod = spark.table("bronze_production")

df_silver = (df_prod
    .filter(F.col("status") == "running")
    .filter(F.col("wind_speed_ms") > 0)
    .withColumn("theoretical_power_kw",
                calc_theoretical_power(F.col("turbine_id"), F.col("wind_speed_ms")))
    .withColumn("performance_ratio",
                F.when(F.col("theoretical_power_kw") > 10,
                       F.round(F.col("power_output_kw") / F.col("theoretical_power_kw"), 4))
                .otherwise(None))
    .withColumn("reading_date", F.to_date("timestamp"))
    .withColumn("reading_hour", F.hour("timestamp"))
    .withColumn("reading_month", F.month("timestamp"))
)

df_silver.write.mode("overwrite").format("delta").saveAsTable("silver_production")
print(f"silver_production : {df_silver.count():,} lignes")
```

**Interpr√©tation** : Chaque ligne a maintenant un `performance_ratio`. Un ratio de 0.95 = la turbine produit 95% de ce qu'elle devrait. Un ratio de 0.55 = elle perd 45% de son potentiel. C'est le thermom√®tre de chaque turbine, mesure par mesure.

### 4.2 ‚Äî Cellule 6 : Les turbines √† ratio < 60%

**Probl√®me terrain** : Le directeur technique veut la liste noire des turbines d√©faillantes.

```python
# === CELLULE 6 : Turbines avec ratio < 60% systematique ===

df_perf = spark.sql("""
    SELECT
        turbine_id,
        COUNT(*) AS nb_readings,
        ROUND(AVG(performance_ratio), 3) AS avg_ratio,
        ROUND(percentile_approx(performance_ratio, 0.1), 3) AS p10_ratio,
        ROUND(percentile_approx(performance_ratio, 0.5), 3) AS p50_ratio,
        ROUND(percentile_approx(performance_ratio, 0.9), 3) AS p90_ratio,
        ROUND(STDDEV(performance_ratio), 4) AS stddev_ratio
    FROM silver_production
    WHERE theoretical_power_kw > 100
    GROUP BY turbine_id
    ORDER BY avg_ratio ASC
""")

df_perf.show(15, truncate=False)

# Marquer les turbines problematiques
df_flagged = df_perf.withColumn("diagnostic",
    F.when(F.col("avg_ratio") < 0.60, "PALES ENDOMMAGEES - Inspection urgente")
    .when(F.col("avg_ratio") < 0.75, "DEGRADATION NAISSANTE - Surveillance renforcee")
    .when(F.col("stddev_ratio") > 0.25, "COMPORTEMENT ERRATIQUE - Verifier capteurs")
    .otherwise("NORMAL")
)

df_flagged.filter(F.col("diagnostic") != "NORMAL").show(truncate=False)
```

**Interpr√©tation** : Les turbines avec `avg_ratio < 0.60` sont les 3 √† pales endommag√©es. Celles entre 0.60 et 0.75 pr√©sentent une d√©gradation naissante ‚Äî si on n'intervient pas maintenant, elles rejoindront la cat√©gorie critique dans 6 mois. Le `stddev_ratio` √©lev√© signale un comportement erratique : la turbine fonctionne bien parfois et mal d'autres fois, ce qui pointe vers un probl√®me intermittent (capteur, pitch, orientation).

### 4.3 ‚Äî Cellule 7 : D√©tection de l'effet d'ombrage

**Probl√®me terrain** : WT-033 sous-performe uniquement quand le vent vient du sud-ouest. C'est la signature classique d'un effet de sillage (wake effect) : une turbine en amont ¬´ vole ¬ª le vent.

```python
# === CELLULE 7 : Analyse directionnelle pour l'ombrage ===

df_wake = spark.sql("""
    SELECT
        turbine_id,
        CASE
            WHEN wind_direction_deg BETWEEN 0 AND 45 THEN 'N-NE'
            WHEN wind_direction_deg BETWEEN 45 AND 90 THEN 'NE-E'
            WHEN wind_direction_deg BETWEEN 90 AND 135 THEN 'E-SE'
            WHEN wind_direction_deg BETWEEN 135 AND 180 THEN 'SE-S'
            WHEN wind_direction_deg BETWEEN 180 AND 225 THEN 'S-SO'
            WHEN wind_direction_deg BETWEEN 225 AND 270 THEN 'SO-O'
            WHEN wind_direction_deg BETWEEN 270 AND 315 THEN 'O-NO'
            ELSE 'NO-N'
        END AS wind_sector,
        ROUND(AVG(performance_ratio), 3) AS avg_ratio,
        COUNT(*) AS nb_readings
    FROM silver_production
    WHERE theoretical_power_kw > 100
    GROUP BY turbine_id, wind_sector
""")

# Pivoter pour voir le ratio par direction pour chaque turbine suspecte
from pyspark.sql.functions import first

df_pivot = (df_wake
    .filter(F.col("turbine_id").isin(["WT-033", "WT-007", "WT-001"]))  # 2 suspectes + 1 saine
    .groupBy("turbine_id")
    .pivot("wind_sector")
    .agg(F.round(F.first("avg_ratio"), 3))
)

df_pivot.show(truncate=False)
```

**Interpr√©tation** : WT-033 montre un ratio normal (0.90+) pour la plupart des directions de vent, mais il chute √† 0.50-0.70 en secteur S-SO et SO-O. C'est l'empreinte de l'ombrage : une turbine en amont lui vole le vent quand il souffle du sud-ouest. La solution est m√©canique (r√©orientation, espacement) ou logicielle (curtailment intelligent). WT-001 (turbine saine) a un ratio stable dans toutes les directions ‚Äî confirmation que le probl√®me est localis√©.

### 4.4 ‚Äî Cellule 8 : Mod√®le ML ‚Äî R√©gression pour confirmer les d√©rives

**Probl√®me terrain** : Le directeur technique veut une preuve statistique, pas juste des moyennes. Un mod√®le de r√©gression entra√Æn√© sur les bonnes turbines permet de pr√©dire ce qu'une turbine *devrait* produire, et de mesurer l'√©cart.

```python
# === CELLULE 8 : Modele ML de detection de derive ===

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# Preparation des features
feature_cols = ["wind_speed_ms", "ambient_temp_c", "wind_direction_deg",
                "reading_hour", "reading_month"]

assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# Entrainer sur les turbines SAINES (ratio > 0.85)
df_train_base = spark.sql("""
    SELECT p.*, r.rated_power_kw, r.region
    FROM silver_production p
    JOIN bronze_turbine_ref r ON p.turbine_id = r.turbine_id
    WHERE p.performance_ratio > 0.85
      AND p.theoretical_power_kw > 100
""").sample(0.1)  # 10% pour la rapidite

df_train = assembler.transform(df_train_base).select("features", "power_output_kw")

# Split train/test
train_data, test_data = df_train.randomSplit([0.8, 0.2], seed=42)

# Gradient Boosted Trees
gbt = GBTRegressor(
    featuresCol="features",
    labelCol="power_output_kw",
    maxIter=50,
    maxDepth=5,
    seed=42
)
model = gbt.fit(train_data)

# Evaluer
predictions = model.transform(test_data)
evaluator = RegressionEvaluator(labelCol="power_output_kw", predictionCol="prediction")
rmse = evaluator.evaluate(predictions, {evaluator.metricName: "rmse"})
r2 = evaluator.evaluate(predictions, {evaluator.metricName: "r2"})
print(f"Modele entraine ‚Äî RMSE: {rmse:.1f} kW, R2: {r2:.3f}")
```

**Interpr√©tation** : Le mod√®le apprend le comportement ¬´ normal ¬ª √† partir des turbines saines. Un R¬≤ > 0.90 signifie que le mod√®le capture bien la relation vent ‚Üí production. On va maintenant l'appliquer sur **toutes** les turbines pour mesurer qui d√©vie.

### 4.5 ‚Äî Cellule 9 : Appliquer le mod√®le et d√©tecter les anomalies

```python
# === CELLULE 9 : Prediction vs reel pour toutes les turbines ===

# Preparer toutes les donnees
df_all = spark.sql("""
    SELECT p.*, r.rated_power_kw
    FROM silver_production p
    JOIN bronze_turbine_ref r ON p.turbine_id = r.turbine_id
    WHERE p.theoretical_power_kw > 100
""")

df_all_features = assembler.transform(df_all)
df_predicted = model.transform(df_all_features)

# Calculer l'ecart par turbine
df_deviation = (df_predicted
    .withColumn("deviation_kw", F.col("power_output_kw") - F.col("prediction"))
    .withColumn("deviation_pct", 
        F.round(F.col("deviation_kw") / F.col("prediction") * 100, 2))
    .groupBy("turbine_id")
    .agg(
        F.round(F.avg("deviation_pct"), 2).alias("avg_deviation_pct"),
        F.round(F.avg("deviation_kw"), 1).alias("avg_deviation_kw"),
        F.count("*").alias("nb_readings"),
        F.round(F.sum(
            F.when(F.col("deviation_pct") < -30, 1).otherwise(0)
        ) / F.count("*") * 100, 1).alias("severe_deviation_pct")
    )
    .orderBy("avg_deviation_pct")
)

df_deviation.show(15, truncate=False)

# Sauvegarder en Silver enrichi
df_deviation.write.mode("overwrite").format("delta").saveAsTable("silver_turbine_anomalies")
```

**Interpr√©tation** : Les turbines avec un `avg_deviation_pct` fortement n√©gatif (ex: -40%) sous-produisent syst√©matiquement par rapport √† ce que le mod√®le pr√©dit dans les m√™mes conditions de vent. Le ML d√©tecte m√™me WT-019 et WT-047 (d√©gradation naissante) que les moyennes simples auraient manqu√©es ‚Äî elles n'apparaissent pas encore sous 60%, mais le mod√®le voit la tendance √† la baisse.

---

## Partie 5 ‚Äî Phase 3 : Synth√®se actionnable (Gold)

### 5.1 ‚Äî Cellule 10 : Tables Gold

```python
# === CELLULE 10 : Tables Gold ===

# --- Gold 1 : KPI quotidiens par turbine ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_daily_turbine_kpi AS
    SELECT
        reading_date,
        p.turbine_id,
        r.region,
        r.model,
        r.rated_power_kw,
        COUNT(*) AS nb_readings,
        ROUND(AVG(p.wind_speed_ms), 1) AS avg_wind_ms,
        ROUND(SUM(p.power_output_kw * 0.25) / 1000, 2) AS production_mwh,
        ROUND(SUM(p.theoretical_power_kw * 0.25) / 1000, 2) AS theoretical_mwh,
        ROUND(AVG(p.performance_ratio), 3) AS avg_performance_ratio,
        ROUND(AVG(p.power_output_kw) / r.rated_power_kw, 3) AS capacity_factor,
        SUM(CASE WHEN p.power_output_kw < 5 AND p.theoretical_power_kw > 100 THEN 1 ELSE 0 END)
            AS ghost_running_count,
        ROUND(SUM(p.theoretical_power_kw * 0.25 - p.power_output_kw * 0.25) / 1000, 2) 
            AS lost_production_mwh
    FROM silver_production p
    JOIN bronze_turbine_ref r ON p.turbine_id = r.turbine_id
    GROUP BY reading_date, p.turbine_id, r.region, r.model, r.rated_power_kw
""")

# --- Gold 2 : Synthese anomalies par turbine (annuel) ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_turbine_diagnostics AS
    SELECT
        p.turbine_id,
        r.region,
        r.model,
        ROUND(AVG(p.performance_ratio), 3) AS avg_ratio,
        ROUND(SUM(CASE WHEN p.performance_ratio < 0.6 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1)
            AS pct_below_60,
        ROUND(SUM(p.theoretical_power_kw * 0.25 - p.power_output_kw * 0.25) / 1000, 0) 
            AS total_lost_mwh,
        ROUND(SUM(p.theoretical_power_kw * 0.25 - p.power_output_kw * 0.25) / 1000 * 75, 0) 
            AS lost_revenue_eur,
        a.avg_deviation_pct AS ml_deviation_pct,
        CASE
            WHEN AVG(p.performance_ratio) < 0.60 THEN 'CRITIQUE - Inspection pales urgente'
            WHEN a.avg_deviation_pct < -25 THEN 'ALERTE - Degradation detectee par ML'
            WHEN SUM(CASE WHEN p.power_output_kw < 5 AND p.theoretical_power_kw > 100 THEN 1 ELSE 0 END) > 100
                THEN 'SUSPECT - Arrets non documentes'
            WHEN AVG(p.performance_ratio) < 0.80 THEN 'SURVEILLANCE - Performance sous-optimale'
            ELSE 'OK'
        END AS diagnostic,
        CASE
            WHEN AVG(p.performance_ratio) < 0.60 THEN 1
            WHEN a.avg_deviation_pct < -25 THEN 2
            WHEN SUM(CASE WHEN p.power_output_kw < 5 AND p.theoretical_power_kw > 100 THEN 1 ELSE 0 END) > 100 THEN 3
            ELSE 4
        END AS priority
    FROM silver_production p
    JOIN bronze_turbine_ref r ON p.turbine_id = r.turbine_id
    LEFT JOIN silver_turbine_anomalies a ON p.turbine_id = a.turbine_id
    WHERE p.theoretical_power_kw > 100
    GROUP BY p.turbine_id, r.region, r.model, a.avg_deviation_pct
    ORDER BY priority, avg_ratio
""")

# --- Gold 3 : Resume financier ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_financial_summary AS
    SELECT
        diagnostic,
        COUNT(*) AS nb_turbines,
        ROUND(SUM(total_lost_mwh), 0) AS total_lost_mwh,
        ROUND(SUM(lost_revenue_eur), 0) AS total_lost_revenue_eur,
        CASE
            WHEN diagnostic LIKE '%pales%' THEN 15000
            WHEN diagnostic LIKE '%ML%' THEN 8000
            WHEN diagnostic LIKE '%Arrets%' THEN 3000
            ELSE 0
        END AS estimated_fix_cost_per_unit,
        CASE
            WHEN diagnostic LIKE '%pales%' THEN 'Inspection drone + reparation composite'
            WHEN diagnostic LIKE '%ML%' THEN 'Diagnostic approfondi + recalibration'
            WHEN diagnostic LIKE '%Arrets%' THEN 'Audit process maintenance + capteurs'
            ELSE 'Aucune action requise'
        END AS recommended_action
    FROM gold_turbine_diagnostics
    GROUP BY diagnostic
    ORDER BY total_lost_revenue_eur DESC
""")

print("Tables Gold creees.")
spark.sql("SELECT * FROM gold_turbine_diagnostics WHERE diagnostic != 'OK'").show(truncate=False)
spark.sql("SELECT * FROM gold_financial_summary").show(truncate=False)
```

**Interpr√©tation du r√©sultat Gold** :

Le tableau `gold_turbine_diagnostics` est **le plan d'action du directeur technique** :

| Diagnostic                          | Ce que √ßa veut dire                                                                                                                                                                                        | Action                                          | Co√ªt              | Gain/an    |
| ----------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ----------------- | ---------- |
| **CRITIQUE - Inspection pales**     | 3 turbines (WT-007, WT-025, WT-038) produisent < 60% de leur potentiel. Les pales sont probablement fissur√©es ou √©rod√©es.                                                                                  | Envoi drone d'inspection + r√©paration composite | ~15 000 ‚Ç¨/turbine | ~180 000 ‚Ç¨ |
| **ALERTE - D√©gradation ML**         | 2 turbines (WT-019, WT-047) montrent une d√©rive invisible aux moyennes mais d√©tect√©e par le mod√®le. Sans intervention, elles passeront en CRITIQUE dans 6 mois.                                            | Diagnostic pr√©ventif + recalibration            | ~8 000 ‚Ç¨/turbine  | ~60 000 ‚Ç¨  |
| **SUSPECT - Arr√™ts non document√©s** | 5 turbines (WT-003, WT-012, WT-019, WT-033, WT-047) d√©clarent tourner mais ne produisent rien, sans maintenance d√©clar√©e. Soit le process de reporting est d√©faillant, soit un d√©faut intermittent existe. | Audit organisationnel + v√©rification capteurs   | ~3 000 ‚Ç¨/turbine  | ~150 000 ‚Ç¨ |
| **Effet d'ombrage**                 | WT-033 perd 30-50% de production quand le vent vient du SO, √† cause de la turbine en amont.                                                                                                                | √âtude d'espacement ou curtailment intelligent   | Variable          | ~60 000 ‚Ç¨  |

**Gain total r√©cup√©rable : ~450 000 ‚Ç¨/an** pour un investissement d'environ 75 000 ‚Ç¨ en inspections et r√©parations.

Le tableau `gold_financial_summary` est **le business case √† pr√©senter au comit√© de direction** : chaque ligne montre le nombre de turbines, la perte annuelle, le co√ªt de correction et le ROI. Le directeur financier voit imm√©diatement que l'investissement est rentabilis√© en 2 mois.

---

## Partie 6 ‚Äî V√©rification finale

### 6.1 ‚Äî Cellule 11 : Les turbines probl√©matiques sont-elles bien d√©tect√©es ?

```python
# === CELLULE 11 : Validation croisee avec les anomalies injectees ===

print("=== TURBINES ATTENDUES ===")
print("Pales endommagees : WT-007, WT-025, WT-038")
print("Arrets non documentes : WT-003, WT-012, WT-019, WT-033, WT-047")
print("Degradation naissante : WT-019, WT-047")
print("Effet d'ombrage : WT-033")
print()

spark.sql("""
    SELECT turbine_id, diagnostic, avg_ratio, ml_deviation_pct, priority
    FROM gold_turbine_diagnostics
    WHERE turbine_id IN ('WT-003','WT-007','WT-012','WT-019','WT-025','WT-033','WT-038','WT-047')
    ORDER BY priority, avg_ratio
""").show(truncate=False)
```

**Interpr√©tation** : Si les 8 turbines inject√©es apparaissent toutes avec un diagnostic non-OK, la cha√Æne analytique fonctionne. Le ML doit en plus d√©tecter WT-019 et WT-047 que les seuils simples auraient rat√©es ‚Äî c'est la valeur ajout√©e du machine learning sur les r√®gles m√©tier manuelles.

---

## R√©capitulatif : ce que la double motorisation Spark SQL / PySpark a permis

| √âtape                             | Outil                         | Pourquoi cet outil                                |
| --------------------------------- | ----------------------------- | ------------------------------------------------- |
| Chargement CSV ‚Üí Delta            | PySpark                       | Gestion de fichiers volumineux                    |
| Vue d'ensemble, agr√©gats          | **Spark SQL**                 | Lisible, rapide, familier pour les analystes      |
| Croisement production/maintenance | **Spark SQL**                 | Jointures classiques                              |
| Courbe de puissance th√©orique     | **PySpark UDF**               | Logique non-lin√©aire (cubique), impossible en SQL |
| Performance ratio par mesure      | **PySpark**                   | Calcul colonne d√©riv√©e avec UDF                   |
| Analyse directionnelle (ombrage)  | **Spark SQL** + pivot PySpark | SQL pour l'agr√©gation, PySpark pour le pivot      |
| Mod√®le ML (GBT r√©gression)        | **PySpark MLlib**             | D√©tection de d√©rives invisibles aux seuils        |
| Tables Gold et KPI                | **Spark SQL**                 | Agr√©gations m√©tier, CASE WHEN, jointures          |

**R√®gle d'or confirm√©e** : Spark SQL pour la d√©couverte et le nettoyage, PySpark d√®s qu'une r√®gle m√©tier non lin√©aire, une UDF ou du ML est n√©cessaire.
