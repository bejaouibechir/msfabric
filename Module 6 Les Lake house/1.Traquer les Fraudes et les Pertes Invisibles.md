# √âtude de Cas n¬∞2 ‚Äì R√©seau de Distribution √âlectrique : Traquer les Fraudes et les Pertes Invisibles

---

## √ânonc√©

### 1. √âtat des lieux : Des gigawatts qui s'√©vaporent dans la nature

√âlectra R√©seau g√®re un r√©seau de distribution de taille m√©tropolitaine : **2 millions de compteurs communicants** (Linky), **15 000 postes de transformation**, et des centaines de capteurs r√©partis sur le terrain. Chaque seconde, des flux de donn√©es remontent : consommation, tension, courant, qualit√© de l'onde.

Pourtant, **7 % de l'√©nergie inject√©e dans le r√©seau n'est jamais factur√©e**. Les pertes techniques (effet Joule, vieillissement des c√¢bles) expliquent une partie, mais une fraction substantielle reste inexpliqu√©e. Les √©quipes terrain soup√ßonnent des branchements frauduleux, des compteurs d√©faillants, ou des erreurs de relev√©.

Les donn√©es existent, mais elles sont √©parpill√©es :

- Bases de donn√©es relationnelles des factures ;
- Fichiers plats des courbes de charge ;
- Logs des interventions techniques ;
- R√©f√©rentiels clients obsol√®tes.

Le rapprochement manuel est impossible : il faudrait croiser des milliards de lignes.

### 2. Probl√©matique : Qui, o√π, quand, combien ?

La direction vous mandate : *¬´ Nous perdons l'√©quivalent de la consommation d'une ville de 100 000 habitants chaque ann√©e. Trouvez-nous ces fuites. ¬ª*

La question pr√©cise devient : **comment d√©tecter, sur un r√©seau de 2 millions de points, les compteurs qui pr√©sentent un profil anormalement bas (fraude) ou une d√©rive m√©trologique, et localiser les tron√ßons de ligne o√π les pertes techniques d√©passent le seuil r√©glementaire ?**

Trois hypoth√®ses s'affrontent :

1. **Fraude active** : d√©rivation, compteur invers√©, bypass ;
2. **D√©faillance m√©trologique** : compteur vieillissant, mesure erron√©e ;
3. **Pertes techniques anormales** : surcharge locale, c√¢ble endommag√©.

L'enjeu financier est colossal : **12 millions d'euros par an** de pertes non justifi√©es.

### 3. Solution attendue : Le Lakehouse comme r√©seau de neurones du r√©seau

L'architecture Medallion permet d'int√©grer, de croiser et de mod√©liser l'ensemble de ces flux h√©t√©rog√®nes.

#### Phase 1 ‚Äì L'exploration (Bronze ‚Äì Silver simple)

Avec **Spark SQL**, vous ing√©rez en quelques minutes :

- les courbes de charge (plus de 5 milliards d'enregistrements) ;
- les r√©f√©rentiels clients et postes sources ;
- les historiques d'interventions.

Premier diagnostic : **12 000 compteurs** ne pr√©sentent aucune consommation depuis plus de 6 mois mais restent actifs dans le syst√®me. Piste de fraude massive.

#### Phase 2 ‚Äì L'investigation approfondie (Silver ‚Äì calculs avanc√©s)

Vous passez en **PySpark** pour calculer, pour chaque compteur, le **profil de consommation attendu** (clustering des usages par type d'abonn√©, saisonnalit√©, temp√©rature).
L'√©cart entre consommation r√©elle et attendue est syst√©matis√©. Un **score de suspicion** est attribu√© via une combinaison de r√®gles m√©tier et d'un mod√®le de d√©tection d'anomalies (Isolation Forest).

R√©sultat : **248 compteurs** pr√©sentent un √©cart > 80 % avec leur profil, sans aucune intervention signal√©e. La fraude est quasi certaine.

Parall√®lement, vous croisez les pertes techniques d√©clar√©es avec les mesures amont/aval des postes de transformation. **15 postes** affichent un diff√©rentiel > 15 % ‚Äì le triple de la norme ‚Äì signant probablement un c√¢ble vieillissant ou une surcharge chronique.

#### Phase 3 ‚Äì La synth√®se actionnable (Gold ‚Äì visualisation)

Les donn√©es sont agr√©g√©es par secteur g√©ographique, par type de fraude, par √©quipe terrain. Un tableau de bord interactif classe les interventions par gain potentiel.

**R√©sultats concrets :**

- **248 fraudes av√©r√©es** apr√®s contr√¥le terrain ‚Üí **3,1 M‚Ç¨/an** r√©cup√©r√©s ;
- **15 r√©fections de c√¢ble** programm√©es ‚Üí **1,8 M‚Ç¨** de pertes √©vit√©es ;
- **40 compteurs d√©faillants** remplac√©s en maintenance pr√©ventive.

**Gain total : 4,9 M‚Ç¨ d√®s la premi√®re ann√©e.**

---

## Mise en ≈ìuvre ‚Äî Atelier pratique

Vous allez construire la solution compl√®te : simuler un r√©seau de 50 000 compteurs (√©chelle r√©duite mais repr√©sentative), d√©tecter les fraudes avec SynapseML Isolation Forest (natif dans Fabric Runtime 1.3), localiser les pertes techniques excessives, et produire les tables Gold pour Power BI.

> üí° **Runtime requis** : **Runtime 1.3** (Spark 3.5, Delta Lake 3.2, Python 3.11) ‚Äî runtime GA par d√©faut. V√©rifiez dans **Workspace Settings** ‚Üí **Data Engineering/Science** ‚Üí **Spark settings**.

> üí° **Performance gratuite** : Activez le **Native Execution Engine**. **Workspace Settings** ‚Üí **Spark settings** ‚Üí onglet **Acceleration** ‚Üí cocher ‚úÖ **Enable native execution engine** ‚Üí **Save**.

---

## Fichiers n√©cessaires

Tous les fichiers sont g√©n√©r√©s par le script ci-dessous. Aucun t√©l√©chargement externe.

---

## Partie 1 ‚Äî G√©n√©rer les donn√©es simul√©es

### 1.1 ‚Äî Script Python : g√©n√©rateur du r√©seau √©lectrique

Cr√©er **`generate_grid_data.py`** sur votre poste local :

```python
import csv
import random
import os
import math
from datetime import datetime, timedelta

random.seed(42)

# ============================================================
# CONFIGURATION
# ============================================================
OUTPUT_DIR = r"D:\msfabric\grid_data"
NB_METERS = 50_000       # 50 000 compteurs (echelle reduite)
NB_SUBSTATIONS = 300      # postes de transformation
YEAR = 2025
# ============================================================

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ----------------------------------------------------------------
# REFERENTIEL POSTES DE TRANSFORMATION
# ----------------------------------------------------------------
REGIONS = ["Ile-de-France", "Rhone-Alpes", "PACA", "Occitanie", "Grand-Est"]
substation_info = {}
for sid in range(1, NB_SUBSTATIONS + 1):
    region = REGIONS[sid % len(REGIONS)]
    substation_info[sid] = {
        "substation_id": f"PST-{sid:04d}",
        "region": region,
        "city": f"Commune-{sid:03d}",
        "capacity_kva": random.choice([250, 400, 630, 1000]),
        "install_year": random.randint(1985, 2020),
        "latitude": round(random.uniform(43.0, 49.5), 4),
        "longitude": round(random.uniform(-1.5, 7.5), 4),
    }

sub_path = os.path.join(OUTPUT_DIR, "substations.csv")
with open(sub_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["substation_id", "region", "city", "capacity_kva",
                "install_year", "latitude", "longitude"])
    for s in substation_info.values():
        w.writerow([s["substation_id"], s["region"], s["city"],
                    s["capacity_kva"], s["install_year"],
                    s["latitude"], s["longitude"]])
print(f"Postes: {sub_path} ({NB_SUBSTATIONS} postes)")

# ----------------------------------------------------------------
# REFERENTIEL COMPTEURS
# ----------------------------------------------------------------
CLIENT_TYPES = ["residential", "commercial", "industrial"]
TARIFFS = ["base", "HP/HC", "tempo"]

# Compteurs frauduleux (248 / 50000 ~ 0.5%)
FRAUD_METERS = set(random.sample(range(1, NB_METERS + 1), 248))
# Compteurs zero-consumption depuis > 6 mois (12 000 sur 2M = 0.6%, scale: 300)
ZERO_METERS = set(random.sample(range(1, NB_METERS + 1), 300)) - FRAUD_METERS
# Compteurs metriquement defaillants (40)
FAULTY_METERS = set(random.sample(range(1, NB_METERS + 1), 40)) - FRAUD_METERS - ZERO_METERS

# Postes avec pertes techniques > 15% (15 / 300 = 5%)
HIGH_LOSS_SUBSTATIONS = set(random.sample(range(1, NB_SUBSTATIONS + 1), 15))

meter_info = {}
for mid in range(1, NB_METERS + 1):
    sub_id = (mid % NB_SUBSTATIONS) + 1
    client_type = random.choices(CLIENT_TYPES, weights=[75, 20, 5])[0]
    meter_info[mid] = {
        "meter_id": f"LNK-{mid:06d}",
        "substation_id": f"PST-{sub_id:04d}",
        "client_type": client_type,
        "contract_power_kva": {"residential": 6, "commercial": 36, "industrial": 120}[client_type],
        "tariff": random.choice(TARIFFS),
        "install_date": f"{random.randint(2015, 2023)}-{random.randint(1,12):02d}-{random.randint(1,28):02d}",
        "is_fraud": mid in FRAUD_METERS,
        "is_zero": mid in ZERO_METERS,
        "is_faulty": mid in FAULTY_METERS,
    }

meter_path = os.path.join(OUTPUT_DIR, "meters.csv")
with open(meter_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["meter_id", "substation_id", "client_type", "contract_power_kva",
                "tariff", "install_date"])
    for m in meter_info.values():
        w.writerow([m["meter_id"], m["substation_id"], m["client_type"],
                    m["contract_power_kva"], m["tariff"], m["install_date"]])
print(f"Compteurs: {meter_path} ({NB_METERS:,} compteurs)")

# ----------------------------------------------------------------
# HISTORIQUE INTERVENTIONS
# ----------------------------------------------------------------
intv_path = os.path.join(OUTPUT_DIR, "interventions.csv")
with open(intv_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["meter_id", "intervention_date", "intervention_type", "result", "technician"])
    for mid in range(1, NB_METERS + 1):
        if random.random() < 0.05:  # 5% ont une intervention
            m = meter_info[mid]
            dt = datetime(YEAR, random.randint(1, 12), random.randint(1, 28))
            i_type = random.choice(["control", "replacement", "reading_error", "complaint"])
            result = random.choice(["ok", "anomaly_found", "meter_replaced", "inconclusive"])
            tech = f"TECH-{random.randint(100, 200)}"
            w.writerow([m["meter_id"], dt.strftime("%Y-%m-%d"), i_type, result, tech])
print(f"Interventions: {intv_path}")

# ----------------------------------------------------------------
# COURBES DE CHARGE MENSUELLES (simplifie: 1 releve/jour/compteur)
# ----------------------------------------------------------------
def base_consumption(client_type, month, day_of_week, hour=12):
    """Consommation de base journaliere (kWh) par type de client."""
    # Saisonnalite
    winter_factor = 1.0 + 0.4 * (month in [11, 12, 1, 2, 3])
    summer_factor = 1.0 + 0.2 * (month in [6, 7, 8])  # clim

    if client_type == "residential":
        base = 12 * winter_factor * summer_factor
        # Weekend = +10%
        if day_of_week >= 5:
            base *= 1.1
    elif client_type == "commercial":
        base = 80 * winter_factor
        # Weekend = -60%
        if day_of_week >= 5:
            base *= 0.4
    else:  # industrial
        base = 500 * winter_factor
        if day_of_week >= 5:
            base *= 0.3
    return base

print("\nGeneration des courbes de charge...")
load_path = os.path.join(OUTPUT_DIR, "load_curves.csv")
total_rows = 0
with open(load_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["meter_id", "reading_date", "consumption_kwh",
                "max_power_kw", "voltage_avg_v", "reactive_power_kvar"])

    # Generer jour par jour pour un trimestre (pour garder une taille raisonnable)
    start = datetime(YEAR, 1, 1)
    end = datetime(YEAR, 3, 31)
    current = start

    while current <= end:
        month = current.month
        dow = current.weekday()

        for mid in range(1, NB_METERS + 1):
            m = meter_info[mid]
            base = base_consumption(m["client_type"], month, dow)

            # --- INJECTION DES ANOMALIES ---
            if m["is_zero"]:
                # Compteur zero : aucune consommation
                consumption = 0.0
                max_power = 0.0
                voltage = round(230 + random.gauss(0, 1), 1)
                reactive = 0.0

            elif m["is_fraud"]:
                # Fraude : consommation 20-40% de la normale
                fraud_factor = random.uniform(0.10, 0.40)
                consumption = base * fraud_factor + random.gauss(0, 1)
                max_power = consumption / 6 * random.uniform(0.8, 1.2)
                voltage = round(230 + random.gauss(0, 3), 1)
                reactive = consumption * random.uniform(0.1, 0.3)

            elif m["is_faulty"]:
                # Compteur defaillant : lectures erratiques
                if random.random() < 0.3:
                    consumption = base * random.uniform(2.0, 5.0)  # pics anormaux
                else:
                    consumption = base * random.uniform(0.5, 0.9)
                max_power = consumption / 4 * random.uniform(0.5, 2.0)
                voltage = round(230 + random.gauss(0, 8), 1)  # tension instable
                reactive = consumption * random.uniform(0.05, 0.5)

            else:
                # Consommation normale
                consumption = base + random.gauss(0, base * 0.15)
                max_power = consumption / 8 * random.uniform(0.8, 1.2)
                voltage = round(230 + random.gauss(0, 2), 1)
                reactive = consumption * random.uniform(0.15, 0.25)

            w.writerow([
                m["meter_id"],
                current.strftime("%Y-%m-%d"),
                round(max(0, consumption), 2),
                round(max(0, max_power), 2),
                voltage,
                round(max(0, reactive), 2)
            ])
            total_rows += 1

        if total_rows % 1_000_000 == 0:
            print(f"  {total_rows:,} lignes generees...")
        current += timedelta(days=1)

print(f"\nCourbes de charge: {load_path}")
print(f"Total: {total_rows:,} lignes")

# ----------------------------------------------------------------
# MESURES POSTES DE TRANSFORMATION (amont/aval)
# ----------------------------------------------------------------
sub_measures_path = os.path.join(OUTPUT_DIR, "substation_measures.csv")
with open(sub_measures_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["substation_id", "reading_date", "energy_injected_kwh",
                "energy_delivered_kwh", "loss_kwh", "loss_pct",
                "max_load_pct", "voltage_deviation_pct"])

    start = datetime(YEAR, 1, 1)
    end = datetime(YEAR, 3, 31)
    current = start

    while current <= end:
        for sid in range(1, NB_SUBSTATIONS + 1):
            s = substation_info[sid]
            base_energy = s["capacity_kva"] * 8 * random.uniform(0.6, 0.9)

            if sid in HIGH_LOSS_SUBSTATIONS:
                # Pertes anormales > 15%
                loss_pct = random.uniform(15, 25)
            else:
                # Pertes normales 3-7%
                loss_pct = random.uniform(3, 7)

            energy_injected = base_energy
            loss = energy_injected * loss_pct / 100
            energy_delivered = energy_injected - loss
            max_load = random.uniform(50, 95)
            voltage_dev = random.uniform(-3, 3)

            if sid in HIGH_LOSS_SUBSTATIONS:
                max_load = random.uniform(80, 105)  # surcharge
                voltage_dev = random.uniform(-6, 6)   # instabilite

            w.writerow([
                f"PST-{sid:04d}",
                current.strftime("%Y-%m-%d"),
                round(energy_injected, 2),
                round(energy_delivered, 2),
                round(loss, 2),
                round(loss_pct, 2),
                round(max_load, 1),
                round(voltage_dev, 2)
            ])
        current += timedelta(days=1)

print(f"Mesures postes: {sub_measures_path}")

# ----------------------------------------------------------------
# RESUME
# ----------------------------------------------------------------
print(f"\n{'='*60}")
print(f"Anomalies injectees:")
print(f"  Compteurs frauduleux:      {len(FRAUD_METERS)} (ecart > 80%)")
print(f"  Compteurs zero:            {len(ZERO_METERS)} (aucune conso > 6 mois)")
print(f"  Compteurs defaillants:     {len(FAULTY_METERS)} (lectures erratiques)")
print(f"  Postes a pertes anormales: {len(HIGH_LOSS_SUBSTATIONS)} (> 15%)")
print(f"{'='*60}")
```

### 1.2 ‚Äî Lancer la g√©n√©ration

```bash
py generate_grid_data.py
```

**Dur√©e** : 3-10 minutes. **Fichiers produits** :

| Fichier                   | Taille estim√©e | Lignes     |
| ------------------------- | -------------- | ---------- |
| `load_curves.csv`         | ~350 Mo        | ~4 500 000 |
| `meters.csv`              | ~3 Mo          | 50 000     |
| `substations.csv`         | ~30 Ko         | 300        |
| `interventions.csv`       | ~100 Ko        | ~2 500     |
| `substation_measures.csv` | ~3 Mo          | ~27 000    |

---

## Partie 2 ‚Äî Lakehouse : Ingestion Bronze

### 2.1 ‚Äî Cr√©er le Lakehouse

1. **app.fabric.microsoft.com** ‚Üí votre workspace
2. **+ New item** ‚Üí **Lakehouse**
3. Nom : **`LH_GridAnalytics`**
4. Cliquer **Create**

### 2.2 ‚Äî Cr√©er la structure et charger les fichiers

1. Dans **LH_GridAnalytics** ‚Üí **Files** ‚Üí cr√©er sous-dossier **`bronze`**
2. Dans **bronze** ‚Üí cr√©er : **`load_curves`**, **`meters`**, **`substations`**, **`interventions`**, **`substation_measures`**
3. Charger chaque CSV dans son dossier correspondant (upload ou OneLake File Explorer pour le gros fichier `load_curves.csv`)

---

## Partie 3 ‚Äî Phase 1 : Exploration (Bronze ‚Üí Silver simple) avec Spark SQL

### 3.1 ‚Äî Cr√©er le Notebook

1. Workspace ‚Üí **+ New item** ‚Üí **Notebook**
2. Nommer : **`NB_Grid_Fraud_Detection`**
3. Attacher au Lakehouse **LH_GridAnalytics**

### 3.2 ‚Äî Cellule 1 : Chargement et cr√©ation des tables Bronze

```python
# === CELLULE 1 : Chargement Bronze ===

# Courbes de charge (gros fichier)
df_load = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/load_curves/load_curves.csv")
)
df_load.write.mode("overwrite").format("delta").saveAsTable("bronze_load_curves")

# Referentiel compteurs
df_meters = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/meters/meters.csv")
)
df_meters.write.mode("overwrite").format("delta").saveAsTable("bronze_meters")

# Referentiel postes
df_sub = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/substations/substations.csv")
)
df_sub.write.mode("overwrite").format("delta").saveAsTable("bronze_substations")

# Interventions
df_intv = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/interventions/interventions.csv")
)
df_intv.write.mode("overwrite").format("delta").saveAsTable("bronze_interventions")

# Mesures postes de transformation
df_sub_meas = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/substation_measures/substation_measures.csv")
)
df_sub_meas.write.mode("overwrite").format("delta").saveAsTable("bronze_substation_measures")

print(f"Courbes de charge : {df_load.count():,} lignes")
print(f"Compteurs         : {df_meters.count():,}")
print(f"Postes             : {df_sub.count()}")
print(f"Interventions     : {df_intv.count()}")
print(f"Mesures postes    : {df_sub_meas.count():,}")
```

### 3.3 ‚Äî Cellule 2 : Premier diagnostic ‚Äî compteurs inactifs

**Probl√®me terrain** : Le responsable fraude veut savoir combien de compteurs sont actifs dans le syst√®me mais ne consomment rien. Ce sont les cibles les plus √©videntes.

**Question m√©tier** : Combien de compteurs n'ont aucune consommation sur les 3 derniers mois ?

```sql
%%sql
-- === CELLULE 2 : Compteurs a consommation nulle ===

SELECT
    m.client_type,
    COUNT(DISTINCT lc.meter_id) AS nb_zero_meters,
    COUNT(DISTINCT m2.meter_id) AS nb_total_meters,
    ROUND(COUNT(DISTINCT lc.meter_id) * 100.0 / COUNT(DISTINCT m2.meter_id), 2) AS pct_zero
FROM bronze_meters m2
LEFT JOIN (
    SELECT meter_id
    FROM bronze_load_curves
    GROUP BY meter_id
    HAVING SUM(consumption_kwh) = 0
) lc ON m2.meter_id = lc.meter_id
JOIN bronze_meters m ON lc.meter_id = m.meter_id
GROUP BY m.client_type
ORDER BY nb_zero_meters DESC
```

**Interpr√©tation** : Un compteur r√©sidentiel √† consommation z√©ro pendant 3 mois en hiver, c'est physiquement suspect. Soit le logement est vide (mais alors le contrat devrait √™tre r√©sili√©), soit le compteur est bypassed. 300 compteurs z√©ro sur 50 000 ‚Üí extrapol√© √† 2 millions de compteurs, √ßa repr√©sente **12 000 cas** ‚Äî exactement le chiffre de l'√©nonc√©. Chaque compteur r√©sidentiel z√©ro = potentiellement 1 500 ‚Ç¨/an de fraude.

### 3.4 ‚Äî Cellule 3 : Profil de consommation moyen par type de client

**Probl√®me terrain** : Avant de d√©tecter les anomalies, il faut √©tablir ce qui est ¬´ normal ¬ª. Le profil moyen par type de client est la r√©f√©rence.

**Question m√©tier** : Quel est le profil de consommation attendu par type de client ?

```sql
%%sql
-- === CELLULE 3 : Profil de reference par type de client et mois ===

SELECT
    m.client_type,
    MONTH(lc.reading_date) AS reading_month,
    COUNT(DISTINCT lc.meter_id) AS nb_meters,
    ROUND(AVG(lc.consumption_kwh), 2) AS avg_consumption_kwh,
    ROUND(percentile_approx(lc.consumption_kwh, 0.1), 2) AS p10_kwh,
    ROUND(percentile_approx(lc.consumption_kwh, 0.5), 2) AS p50_kwh,
    ROUND(percentile_approx(lc.consumption_kwh, 0.9), 2) AS p90_kwh,
    ROUND(STDDEV(lc.consumption_kwh), 2) AS stddev_kwh
FROM bronze_load_curves lc
JOIN bronze_meters m ON lc.meter_id = m.meter_id
WHERE lc.consumption_kwh > 0
GROUP BY m.client_type, MONTH(lc.reading_date)
ORDER BY m.client_type, reading_month
```

**Interpr√©tation** : On obtient la ¬´ norme ¬ª pour chaque type de client et chaque mois. Un r√©sidentiel qui consomme 12 kWh/jour en janvier (moyenne attendue ~16 kWh en hiver) est d√©j√† bas. Un r√©sidentiel √† 3 kWh/jour est anormalement bas ‚Üí signal de fraude. Le P10 (10e percentile) est le seuil en dessous duquel on est dans les 10% les plus bas ‚Äî c'est notre premier filtre.

### 3.5 ‚Äî Cellule 4 : D√©tection des compteurs anormalement bas

**Probl√®me terrain** : L'√©quipe fraude veut la liste des compteurs dont la consommation est significativement en dessous du profil normal de leur cat√©gorie.

**Question m√©tier** : Quels compteurs consomment moins de 40% de la moyenne de leur cat√©gorie ?

```sql
%%sql
-- === CELLULE 4 : Compteurs suspects ‚Äî sous-consommation ===

CREATE OR REPLACE TEMP VIEW v_meter_stats AS
SELECT
    lc.meter_id,
    m.client_type,
    m.substation_id,
    m.contract_power_kva,
    ROUND(AVG(lc.consumption_kwh), 2) AS avg_daily_kwh,
    ROUND(STDDEV(lc.consumption_kwh), 2) AS stddev_daily_kwh,
    COUNT(*) AS nb_readings,
    ROUND(AVG(lc.voltage_avg_v), 1) AS avg_voltage,
    ROUND(STDDEV(lc.voltage_avg_v), 2) AS stddev_voltage
FROM bronze_load_curves lc
JOIN bronze_meters m ON lc.meter_id = m.meter_id
GROUP BY lc.meter_id, m.client_type, m.substation_id, m.contract_power_kva;

CREATE OR REPLACE TEMP VIEW v_type_benchmarks AS
SELECT
    client_type,
    ROUND(AVG(avg_daily_kwh), 2) AS type_avg_kwh,
    ROUND(STDDEV(avg_daily_kwh), 2) AS type_stddev_kwh
FROM v_meter_stats
WHERE avg_daily_kwh > 0
GROUP BY client_type;

SELECT
    s.meter_id,
    s.client_type,
    s.substation_id,
    s.avg_daily_kwh,
    b.type_avg_kwh,
    ROUND(s.avg_daily_kwh / b.type_avg_kwh * 100, 1) AS pct_of_normal,
    s.stddev_voltage,
    CASE
        WHEN s.avg_daily_kwh = 0 THEN 'ZERO_CONSUMPTION'
        WHEN s.avg_daily_kwh / b.type_avg_kwh < 0.20 THEN 'CRITICAL_LOW'
        WHEN s.avg_daily_kwh / b.type_avg_kwh < 0.40 THEN 'SUSPICIOUS_LOW'
        WHEN s.stddev_voltage > 5 THEN 'VOLTAGE_ANOMALY'
        ELSE 'NORMAL'
    END AS initial_flag
FROM v_meter_stats s
JOIN v_type_benchmarks b ON s.client_type = b.client_type
WHERE s.avg_daily_kwh / b.type_avg_kwh < 0.40
   OR s.avg_daily_kwh = 0
   OR s.stddev_voltage > 5
ORDER BY pct_of_normal ASC
```

**Interpr√©tation** : Chaque ligne est un compteur suspect. `pct_of_normal = 15%` signifie que le compteur consomme 85% de moins que la normale de sa cat√©gorie. `ZERO_CONSUMPTION` = aucune consommation enregistr√©e. `CRITICAL_LOW` = quasi-certainement une fraude. `VOLTAGE_ANOMALY` = le compteur voit des tensions erratiques ‚Üí probable d√©faillance m√©trologique. L'√©quipe fraude a maintenant sa **shortlist** pour les visites terrain.

---

## Partie 4 ‚Äî Phase 2 : Investigation approfondie (Silver avanc√©) avec PySpark

### 4.1 ‚Äî Cellule 5 : Calcul du profil attendu et de l'√©cart par compteur

**Probl√®me terrain** : Les seuils fixes (< 40% de la moyenne) sont grossiers. Il faut un profil attendu personnalis√© par compteur, tenant compte du type, de la saison et du jour de la semaine.

**Question m√©tier** : Quel est l'√©cart entre consommation r√©elle et profil attendu pour chaque compteur ?

```python
# === CELLULE 5 : Profil attendu et deviation par compteur ===

from pyspark.sql import functions as F
from pyspark.sql.window import Window

df_load = spark.table("bronze_load_curves")
df_meters = spark.table("bronze_meters")

# Enrichir avec type client et decomposition temporelle
df_enriched = (df_load
    .join(df_meters.select("meter_id", "client_type", "contract_power_kva", "substation_id"),
          on="meter_id", how="inner")
    .withColumn("reading_month", F.month("reading_date"))
    .withColumn("day_of_week", F.dayofweek("reading_date"))
    .withColumn("is_weekend", F.when(F.col("day_of_week").isin([1, 7]), 1).otherwise(0))
)

# Calculer la consommation mediane par (client_type, month, is_weekend) comme reference
w_ref = Window.partitionBy("client_type", "reading_month", "is_weekend")

df_with_ref = (df_enriched
    .withColumn("ref_median_kwh",
                F.percentile_approx("consumption_kwh", 0.5).over(w_ref))
    .withColumn("deviation_pct",
                F.when(F.col("ref_median_kwh") > 0,
                       F.round((F.col("consumption_kwh") - F.col("ref_median_kwh"))
                               / F.col("ref_median_kwh") * 100, 2))
                .otherwise(None))
)

# Agreger par compteur
df_meter_profile = (df_with_ref
    .groupBy("meter_id", "client_type", "substation_id", "contract_power_kva")
    .agg(
        F.round(F.avg("consumption_kwh"), 2).alias("avg_consumption_kwh"),
        F.round(F.avg("ref_median_kwh"), 2).alias("avg_expected_kwh"),
        F.round(F.avg("deviation_pct"), 2).alias("avg_deviation_pct"),
        F.round(F.stddev("consumption_kwh"), 2).alias("stddev_consumption"),
        F.round(F.avg("voltage_avg_v"), 1).alias("avg_voltage"),
        F.round(F.stddev("voltage_avg_v"), 2).alias("stddev_voltage"),
        F.round(F.avg("max_power_kw"), 2).alias("avg_max_power_kw"),
        F.round(F.avg("reactive_power_kvar"), 2).alias("avg_reactive_kvar"),
        F.count("*").alias("nb_readings"),
        F.sum(F.when(F.col("consumption_kwh") == 0, 1).otherwise(0)).alias("zero_days"),
    )
)

df_meter_profile.write.mode("overwrite").format("delta").saveAsTable("silver_meter_profiles")
print(f"silver_meter_profiles: {df_meter_profile.count():,} compteurs")
```

**Interpr√©tation** : Chaque compteur a maintenant un profil personnalis√©. Le `avg_deviation_pct` de -75% signifie que le compteur consomme 75% de moins que la m√©diane de sa cat√©gorie dans les m√™mes conditions (m√™me mois, m√™me type de jour). C'est beaucoup plus pr√©cis qu'une simple comparaison √† la moyenne globale : un commercial ferm√© le week-end n'est pas frauduleux, mais un r√©sidentiel √† -75% en semaine d'hiver l'est probablement.

### 4.2 ‚Äî Cellule 6 : Isolation Forest ‚Äî D√©tection d'anomalies multivari√©e

**Probl√®me terrain** : La fraude sophistiqu√©e ne se limite pas √† une sous-consommation. Un fraudeur peut consommer normalement mais avec un profil de tension, de puissance r√©active ou de puissance max incoh√©rent. Il faut une d√©tection multivari√©e.

**Question m√©tier** : Quels compteurs pr√©sentent un comportement globalement anormal sur l'ensemble de leurs indicateurs ?

```python
# === CELLULE 6 : Isolation Forest (SynapseML natif dans Fabric) ===

from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml import Pipeline
from synapse.ml.isolationforest import IsolationForest

# Preparer les features pour l'Isolation Forest
df_features = spark.table("silver_meter_profiles").filter(F.col("nb_readings") > 30)

feature_cols = [
    "avg_consumption_kwh",
    "avg_deviation_pct",
    "stddev_consumption",
    "avg_voltage",
    "stddev_voltage",
    "avg_max_power_kw",
    "avg_reactive_kvar",
    "zero_days",
]

# Remplacer les nulls par 0
for col_name in feature_cols:
    df_features = df_features.withColumn(col_name,
        F.coalesce(F.col(col_name), F.lit(0.0)))

# Pipeline : VectorAssembler + StandardScaler + IsolationForest
assembler = VectorAssembler(inputCols=feature_cols, outputCol="raw_features")
scaler = StandardScaler(inputCol="raw_features", outputCol="features",
                        withStd=True, withMean=True)

isolation_forest = (IsolationForest()
    .setNumEstimators(100)
    .setMaxSamples(256)
    .setMaxFeatures(1.0)
    .setBootstrap(False)
    .setFeaturesCol("features")
    .setPredictionCol("is_anomaly")
    .setScoreCol("anomaly_score")
    .setContamination(0.01)       # 1% d'anomalies attendues
    .setContaminationError(0.005)
    .setRandomSeed(42)
)

pipeline = Pipeline(stages=[assembler, scaler, isolation_forest])
model = pipeline.fit(df_features)
df_scored = model.transform(df_features)

# Sauvegarder les resultats
df_results = (df_scored
    .select(
        "meter_id", "client_type", "substation_id", "contract_power_kva",
        "avg_consumption_kwh", "avg_expected_kwh", "avg_deviation_pct",
        "stddev_consumption", "avg_voltage", "stddev_voltage",
        "zero_days", "anomaly_score", "is_anomaly"
    )
)

df_results.write.mode("overwrite").format("delta").saveAsTable("silver_anomaly_scores")

# Stats
total = df_results.count()
anomalies = df_results.filter(F.col("is_anomaly") == 1).count()
print(f"Compteurs analyses: {total:,}")
print(f"Anomalies detectees: {anomalies}")
print(f"Taux: {anomalies/total*100:.2f}%")
```

> ‚ö†Ô∏è **Pi√®ge** : `synapse.ml.isolationforest` est pr√©install√© dans Fabric Runtime 1.3. Pas besoin de `%pip install`. Si vous recevez un `ModuleNotFoundError`, v√©rifiez que votre Runtime est bien 1.3 (pas 1.1 ou 1.2 qui avaient une version diff√©rente de SynapseML).

**Interpr√©tation** : L'Isolation Forest analyse 8 dimensions simultan√©ment. Un compteur peut avoir une consommation "normale" mais une combinaison tension + puissance r√©active incoh√©rente ‚Üí c'est la signature d'un compteur qui a √©t√© manipul√© (c√¢blage invers√©, shunt magn√©tique). Le `anomaly_score` quantifie le degr√© d'anomalie : plus c'est √©lev√©, plus le comportement est aberrant.

### 4.3 ‚Äî Cellule 7 : Classifier les anomalies par type

**Probl√®me terrain** : L'√©quipe terrain a besoin de savoir **quel type** de probl√®me chercher avant d'aller sur place. Envoyer un agent anti-fraude pour un compteur d√©faillant est une perte de temps.

```python
# === CELLULE 7 : Classification des anomalies par type ===

df_anomalies = spark.table("silver_anomaly_scores")

# Croiser avec les interventions pour identifier les cas deja connus
df_interventions = spark.table("bronze_interventions")

df_classified = (df_anomalies
    .withColumn("fraud_type",
        F.when(F.col("zero_days") > 60, "ZERO_CONSUMPTION - Bypass probable")
        .when((F.col("avg_deviation_pct") < -60) & (F.col("stddev_voltage") < 3),
              "FRAUD_ACTIVE - Derivation/shunt")
        .when((F.col("avg_deviation_pct") < -40) & (F.col("stddev_voltage") < 3),
              "FRAUD_SUSPECTED - Sous-comptage")
        .when(F.col("stddev_voltage") > 5,
              "METER_FAULTY - Defaillance metrologique")
        .when((F.col("stddev_consumption") > F.col("avg_consumption_kwh") * 0.8),
              "METER_ERRATIC - Lectures incoherentes")
        .when(F.col("is_anomaly") == 1,
              "ML_ANOMALY - Profil atypique multivarie")
        .otherwise("NORMAL"))
    # Verifier si une intervention existe deja
    .join(
        df_interventions.select("meter_id",
            F.col("intervention_date").alias("last_intervention"),
            F.col("result").alias("intervention_result")),
        on="meter_id", how="left")
    .withColumn("already_investigated",
        F.when(F.col("last_intervention").isNotNull(), True).otherwise(False))
)

# Filtrer les anomalies
df_suspects = df_classified.filter(F.col("fraud_type") != "NORMAL")
df_suspects.write.mode("overwrite").format("delta").saveAsTable("silver_fraud_suspects")

# Resume
df_suspects.groupBy("fraud_type").agg(
    F.count("*").alias("nb_compteurs"),
    F.sum(F.when(F.col("already_investigated"), 1).otherwise(0)).alias("deja_investigues"),
    F.round(F.avg("avg_deviation_pct"), 1).alias("avg_deviation_pct"),
    F.round(F.avg("anomaly_score"), 4).alias("avg_anomaly_score")
).orderBy(F.desc("nb_compteurs")).show(truncate=False)
```

**Interpr√©tation** : L'√©quipe terrain sait maintenant exactement quoi chercher. `FRAUD_ACTIVE` ‚Üí envoyer un agent anti-fraude avec un √©quipement de v√©rification. `METER_FAULTY` ‚Üí envoyer un technicien avec un compteur de remplacement. `ML_ANOMALY` ‚Üí le ML a d√©tect√© quelque chose que les r√®gles simples n'ont pas vu ‚Äî investigation approfondie. `already_investigated = True` ‚Üí ne pas y retourner inutilement.

### 4.4 ‚Äî Cellule 8 : D√©tection des postes √† pertes techniques anormales

**Probl√®me terrain** : Parall√®lement aux fraudes individuelles, certains tron√ßons du r√©seau perdent beaucoup trop d'√©nergie. La norme europ√©enne accepte 5-7% de pertes techniques. Au-del√† de 15%, c'est un c√¢ble endommag√© ou une surcharge chronique.

```python
# === CELLULE 8 : Postes de transformation a pertes excessives ===

df_sub_measures = spark.table("bronze_substation_measures")
df_substations = spark.table("bronze_substations")

df_sub_analysis = (df_sub_measures
    .groupBy("substation_id")
    .agg(
        F.round(F.avg("loss_pct"), 2).alias("avg_loss_pct"),
        F.round(F.max("loss_pct"), 2).alias("max_loss_pct"),
        F.round(F.avg("max_load_pct"), 1).alias("avg_load_pct"),
        F.round(F.max("max_load_pct"), 1).alias("max_load_pct"),
        F.round(F.avg("voltage_deviation_pct"), 2).alias("avg_voltage_dev"),
        F.round(F.avg("energy_injected_kwh"), 0).alias("avg_energy_injected"),
        F.round(F.sum("loss_kwh"), 0).alias("total_loss_kwh"),
        F.count("*").alias("nb_readings")
    )
    .join(df_substations, on="substation_id", how="inner")
    .withColumn("risk_level",
        F.when(F.col("avg_loss_pct") > 15, "CRITIQUE - Refection urgente")
        .when(F.col("avg_loss_pct") > 10, "ELEVE - Inspection prioritaire")
        .when(F.col("avg_load_pct") > 90, "SURCHARGE - Renforcement necessaire")
        .otherwise("NORMAL"))
    .withColumn("estimated_annual_loss_eur",
        F.round(F.col("total_loss_kwh") * 4 * 0.15, 0))  # x4 pour annualiser, 0.15 EUR/kWh
)

df_sub_analysis.write.mode("overwrite").format("delta").saveAsTable("silver_substation_losses")

# Top 20 des postes les plus critiques
(df_sub_analysis
    .filter(F.col("risk_level") != "NORMAL")
    .orderBy(F.desc("avg_loss_pct"))
    .select("substation_id", "region", "city", "capacity_kva", "install_year",
            "avg_loss_pct", "max_load_pct", "total_loss_kwh",
            "estimated_annual_loss_eur", "risk_level")
    .show(20, truncate=False))
```

**Interpr√©tation** : Les postes avec `avg_loss_pct > 15%` sont les ¬´ tuyaux perc√©s ¬ª du r√©seau. Un poste qui injecte 5 000 kWh/jour mais n'en d√©livre que 4 000 ‚Üí 1 000 kWh/jour partent en fum√©e, litt√©ralement (effet Joule dans un c√¢ble endommag√© ou surcharg√©). `estimated_annual_loss_eur` chiffre le gaspillage : souvent 50 000-100 000 ‚Ç¨ par an par poste. L'investissement pour remplacer un c√¢ble (30 000-80 000 ‚Ç¨) est rentabilis√© en un an.

---

## Partie 5 ‚Äî Phase 3 : Synth√®se actionnable (Gold)

### 5.1 ‚Äî Cellule 9 : Tables Gold

```python
# === CELLULE 9 : Tables Gold ===

# --- Gold 1 : Plan d'intervention fraude ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_fraud_action_plan AS
    SELECT
        fraud_type,
        COUNT(*) AS nb_compteurs,
        SUM(CASE WHEN already_investigated THEN 1 ELSE 0 END) AS deja_investigues,
        COUNT(*) - SUM(CASE WHEN already_investigated THEN 1 ELSE 0 END) AS a_investiguer,
        ROUND(AVG(avg_deviation_pct), 1) AS avg_deviation_pct,
        ROUND(AVG(avg_consumption_kwh), 1) AS avg_consumption_kwh,
        ROUND(AVG(avg_expected_kwh), 1) AS avg_expected_kwh,
        ROUND(SUM(avg_expected_kwh - avg_consumption_kwh) * 90 * 0.15, 0) AS estimated_loss_q1_eur,
        ROUND(SUM(avg_expected_kwh - avg_consumption_kwh) * 365 * 0.15, 0) AS estimated_loss_annual_eur,
        CASE
            WHEN fraud_type LIKE '%FRAUD_ACTIVE%' THEN 'Agent anti-fraude + equipement verification'
            WHEN fraud_type LIKE '%ZERO%' THEN 'Agent anti-fraude + verification terrain'
            WHEN fraud_type LIKE '%SUSPECTED%' THEN 'Agent anti-fraude standard'
            WHEN fraud_type LIKE '%FAULTY%' THEN 'Technicien + compteur remplacement'
            WHEN fraud_type LIKE '%ERRATIC%' THEN 'Technicien diagnostic'
            ELSE 'Investigation approfondie'
        END AS team_action,
        CASE
            WHEN fraud_type LIKE '%FRAUD_ACTIVE%' THEN 1
            WHEN fraud_type LIKE '%ZERO%' THEN 2
            WHEN fraud_type LIKE '%SUSPECTED%' THEN 3
            WHEN fraud_type LIKE '%FAULTY%' THEN 4
            ELSE 5
        END AS priority
    FROM silver_fraud_suspects
    GROUP BY fraud_type
    ORDER BY priority
""")

# --- Gold 2 : Plan d'intervention reseau ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_network_action_plan AS
    SELECT
        substation_id,
        region,
        city,
        capacity_kva,
        install_year,
        avg_loss_pct,
        max_load_pct,
        total_loss_kwh,
        estimated_annual_loss_eur,
        risk_level,
        CASE
            WHEN avg_loss_pct > 15 THEN 'Refection cable + inspection transformateur'
            WHEN avg_load_pct > 90 THEN 'Renforcement ou repartition charge'
            ELSE 'Inspection standard'
        END AS action,
        CASE
            WHEN avg_loss_pct > 15 THEN ROUND(capacity_kva * 50, 0)
            WHEN avg_load_pct > 90 THEN ROUND(capacity_kva * 30, 0)
            ELSE 5000
        END AS estimated_fix_cost_eur
    FROM silver_substation_losses
    WHERE risk_level != 'NORMAL'
    ORDER BY estimated_annual_loss_eur DESC
""")

# --- Gold 3 : Resume executif ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_executive_summary AS
    SELECT 'Fraudes et sous-comptage' AS category,
        (SELECT SUM(a_investiguer) FROM gold_fraud_action_plan) AS nb_actions,
        (SELECT SUM(estimated_loss_annual_eur) FROM gold_fraud_action_plan) AS potential_recovery_eur,
        'Equipes anti-fraude' AS team
    UNION ALL
    SELECT 'Pertes techniques reseau' AS category,
        (SELECT COUNT(*) FROM gold_network_action_plan) AS nb_actions,
        (SELECT SUM(estimated_annual_loss_eur) FROM gold_network_action_plan) AS potential_recovery_eur,
        'Equipes maintenance reseau' AS team
    UNION ALL
    SELECT 'Compteurs defaillants' AS category,
        (SELECT nb_compteurs FROM gold_fraud_action_plan WHERE fraud_type LIKE '%FAULTY%') AS nb_actions,
        (SELECT estimated_loss_annual_eur FROM gold_fraud_action_plan WHERE fraud_type LIKE '%FAULTY%') AS potential_recovery_eur,
        'Techniciens metrologie' AS team
""")

print("=== PLAN D'ACTION FRAUDE ===")
spark.sql("SELECT * FROM gold_fraud_action_plan").show(truncate=False)

print("\n=== PLAN D'ACTION RESEAU ===")
spark.sql("SELECT * FROM gold_network_action_plan").show(truncate=False)

print("\n=== RESUME EXECUTIF ===")
spark.sql("SELECT * FROM gold_executive_summary").show(truncate=False)
```

**Interpr√©tation du r√©sultat Gold** :

Le tableau `gold_fraud_action_plan` est **le tableau de pilotage du directeur des pertes** :

| Cat√©gorie            | Ce que √ßa veut dire concr√®tement                                                                                                                                                                                                                  |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **FRAUD_ACTIVE**     | Compteurs qui consomment 10-30% de la normale avec une tension stable ‚Üí d√©rivation (le courant passe autour du compteur). Envoi d'un agent anti-fraude √©quip√© d'une pince amp√®rem√©trique pour mesurer le courant r√©el vs le courant comptabilis√©. |
| **ZERO_CONSUMPTION** | Contrats actifs, compteurs muets ‚Üí bypass total ou compteur d√©branch√©. V√©rification physique du branchement.                                                                                                                                      |
| **FRAUD_SUSPECTED**  | Consommation √† 40-60% de la normale ‚Üí possible sous-comptage par manipulation du disjoncteur ou de l'index. Contr√¥le m√©trique approfondi.                                                                                                         |
| **METER_FAULTY**     | Lectures erratiques avec tension instable ‚Üí pas une fraude mais un compteur d√©faillant qui sous-enregistre. Remplacement pr√©ventif.                                                                                                               |
| **ML_ANOMALY**       | Le mod√®le Isolation Forest a d√©tect√© un profil multivari√©e atypique que les r√®gles simples ont rat√© ‚Üí investigation approfondie par un analyste senior.                                                                                           |

Le tableau `gold_network_action_plan` priorise les **15 postes** √† pertes anormales par gain financier d√©croissant. Le directeur r√©seau voit imm√©diatement que les 5 premiers postes concentrent 60% des pertes ‚Üí c'est l√† qu'il faut mettre les √©quipes en premier.

Le `gold_executive_summary` est **le slide pour le comit√© de direction** : 3 lignes, 3 chiffres. Fraudes = X M‚Ç¨ r√©cup√©rables. R√©seau = Y M‚Ç¨ de pertes √©vitables. Compteurs = Z remplacements. Total = **4,9 M‚Ç¨/an** de gain potentiel.

---

## Partie 6 ‚Äî V√©rification finale

### 6.1 ‚Äî Cellule 10 : Validation crois√©e avec les anomalies inject√©es

```python
# === CELLULE 10 : Validation ===

print("=== ANOMALIES INJECTEES ===")
print("Compteurs frauduleux: 248 (ecart > 80%)")
print("Compteurs zero:       300 (aucune conso)")
print("Compteurs defaillants: 40 (lectures erratiques)")
print("Postes a pertes > 15%: 15")
print()

# Verifier les compteurs detectes
df_suspects = spark.table("silver_fraud_suspects")
df_meters = spark.table("bronze_meters")

# Reconstruire les flags du generateur via les meter_id
# (Les 248 premiers fraud, 300 zero, 40 faulty sont dans des ranges previsibles)
print(f"Total suspects detectes: {df_suspects.count()}")
print()

# Verifier les postes
df_network = spark.table("gold_network_action_plan")
print(f"Postes critiques detectes: {df_network.filter(F.col('avg_loss_pct') > 15).count()}")
print(f"(attendu: 15)")
```

---

## R√©capitulatif : ce que chaque outil a apport√©

| √âtape                         | Outil                             | Pourquoi                                                 |
| ----------------------------- | --------------------------------- | -------------------------------------------------------- |
| Chargement CSV ‚Üí Delta        | PySpark                           | Volume de donn√©es important                              |
| Compteurs inactifs, profils   | **Spark SQL** (`%%sql`)           | Agr√©gations classiques, lisible                          |
| Compteurs suspects par seuils | **Spark SQL**                     | Comparaisons, CASE WHEN                                  |
| Profil attendu personnalis√©   | **PySpark** Window functions      | `percentile_approx` over partition impossible en SQL pur |
| Isolation Forest multivari√©e  | **SynapseML** (natif Runtime 1.3) | D√©tection d'anomalies 8 dimensions                       |
| Classification par type       | **PySpark**                       | Logique conditionnelle complexe                          |
| Pertes postes transformation  | **PySpark** + jointures           | Calculs financiers d√©riv√©s                               |
| Tables Gold et KPI            | **Spark SQL**                     | Agr√©gations finales pour Power BI                        |

**R√®gle confirm√©e** : Spark SQL pour l'exploration et les agr√©gats. PySpark + SynapseML d√®s qu'on entre dans le ML et les window functions avanc√©es.

---

**Prochaine √©tape** ‚Üí √âtude de Cas n¬∞3 : Parc Solaire ‚Äî Pr√©dire la production et d√©tecter les panneaux paresseux avec 120 000 panneaux et des UDF de performance photovolta√Øque.
