# √âtude de Cas n¬∞3 ‚Äì Parc Solaire : Pr√©dire la Production et D√©tecter les Panneaux Paresseux

---

## √ânonc√©

### 1. √âtat des lieux : Le potentiel solaire sous-exploit√©

Soleil Vert exploite **25 centrales photovolta√Øques** r√©parties dans le sud de la France, totalisant **120 000 panneaux**. Chaque centrale est √©quip√©e d'onduleurs et de capteurs qui remontent :

- production DC et AC (W) ;
- irradiation solaire (W/m¬≤) ;
- temp√©rature des panneaux et ambiante ;
- tensions, courants, alarmes.

L'entreprise affiche un **rendement global de 82 %** par rapport au mod√®le th√©orique. Un √©cart de 3 % semble tol√©rable, mais 5 points de moins repr√©sentent **800 000 ‚Ç¨ de manque √† gagner** chaque ann√©e. La direction technique soup√ßonne des **panneaux sous-performants** (salissures, micro-fissures, vieillissement pr√©matur√©) et des **onduleurs en sous-r√©gime**.

Probl√®me : avec 120 000 panneaux et des donn√©es par minute, on atteint **plus de 60 millions de lignes par an**. Impossible de tout analyser manuellement.

### 2. Probl√©matique : Trouver les 5 % de panneaux qui causent 80 % des pertes

Le directeur d'exploitation vous lance le d√©fi : *¬´ Je sais que certains panneaux ne donnent pas leur pleine puissance. Mais je ne peux pas envoyer une √©quipe sur 25 centrales pour v√©rifier 120 000 panneaux un par un. Priorisez-moi les interventions. ¬ª*

La question : **quels sont les panneaux ou cha√Ænes de panneaux dont la production est significativement inf√©rieure √† la production attendue (donn√©e par la courbe de performance standard) ?**

Hypoth√®ses en pr√©sence :

1. **Encrassement** : panneaux sales, masques, oiseaux ;
2. **Vieillissement pr√©matur√©** : d√©gradation des cellules ;
3. **D√©faut onduleur** : tracking du point de puissance maximum inefficace ;
4. **Erreur de mesure** : capteur d√©faillant.

### 3. Solution attendue : Le Lakehouse, centrale de calcul de l'√©nergie solaire

L'approche Medallion permet de traiter ces 60 millions de lignes et d'extraire une liste d'interventions prioris√©es.

#### Phase 1 ‚Äì L'exploration (Bronze ‚Äì Silver simple)

**Spark SQL** ing√®re les donn√©es brutes de toutes les centrales. La qualit√© des donn√©es est h√©t√©rog√®ne : certains champs d'irradiation sont manquants, des horodatages sont en fuseaux diff√©rents, des onduleurs remontent des codes d'erreur au lieu de valeurs.
Nettoyage rapide : d√©doublonnage, filtrage des plages horaires nocturnes (production nulle normale), conversion des fuseaux.

#### Phase 2 ‚Äì L'investigation approfondie (Silver ‚Äì calculs avanc√©s)

Le c≈ìur du probl√®me r√©side dans le calcul de la **production th√©orique**. Elle d√©pend de l'irradiation, de la temp√©rature, de l'angle d'inclinaison, de l'ombrage et du mod√®le du panneau. Une simple formule lin√©aire est insuffisante.

Avec **PySpark**, vous d√©veloppez une UDF qui, pour chaque minute et chaque panneau, estime la puissance DC attendue √† partir des caract√©ristiques techniques et des conditions r√©elles. Vous calculez ensuite le **ratio de performance** minute par minute.

R√©sultat : **6 000 panneaux** (5 % du parc) affichent un ratio < 75 % sur les heures d'ensoleillement maximal. Le manque √† gagner est quantifi√© : **480 000 ‚Ç¨/an**.

En poussant l'analyse, vous clust√©risez ces panneaux par type de d√©faut :

- **forte corr√©lation avec le temps depuis le dernier nettoyage** ‚Üí encrassement ;
- **d√©rive lente et irr√©versible** ‚Üí vieillissement ;
- **baisse brutale, synchrone avec un onduleur** ‚Üí d√©faut de tracking.

#### Phase 3 ‚Äì La synth√®se actionnable (Gold ‚Äì visualisation)

Un tableau de bord classe les interventions :

- **nettoyage prioritaire** sur 4 centrales (gain 280 000 ‚Ç¨/an) ;
- **remplacement de 340 panneaux** d√©grad√©s (co√ªt 70 000 ‚Ç¨, gain 150 000 ‚Ç¨/an) ;
- **recalibrage de 12 onduleurs** (gain 50 000 ‚Ç¨/an).

**ROI de l'analyse : plus de 1000 % sur la premi√®re ann√©e.**

---

## Mise en ≈ìuvre ‚Äî Atelier pratique

Vous allez construire la solution compl√®te : simuler 25 centrales / 2 500 cha√Ænes de panneaux (√©chelle r√©duite), calculer les ratios de performance avec une UDF photovolta√Øque, clust√©riser les d√©fauts avec KMeans (SparkML natif), et produire les tables Gold.

> üí° **Runtime requis** : **Runtime 1.3** (Spark 3.5, Delta Lake 3.2, Python 3.11). V√©rifiez dans **Workspace Settings** ‚Üí **Data Engineering/Science** ‚Üí **Spark settings**.

> üí° **Native Execution Engine** : Activez-le pour acc√©l√©rer le traitement. **Workspace Settings** ‚Üí **Spark settings** ‚Üí **Acceleration** ‚Üí ‚úÖ **Enable native execution engine**.

---

## Partie 1 ‚Äî G√©n√©rer les donn√©es simul√©es

### 1.1 ‚Äî Script Python : g√©n√©rateur du parc solaire

Cr√©er **`generate_solar_park_data.py`** sur votre poste local :

```python
import csv
import random
import os
import math
from datetime import datetime, timedelta

random.seed(42)

# ============================================================
# CONFIGURATION
# ============================================================
OUTPUT_DIR = r"D:\msfabric\solar_data"
NB_PLANTS = 25           # centrales
NB_STRINGS = 2500        # chaines de panneaux (100 par centrale)
PANELS_PER_STRING = 48   # 48 panneaux par chaine (total 120 000)
YEAR = 2025
INTERVAL_MINUTES = 15    # une mesure toutes les 15 min
SIMULATION_DAYS = 90     # Q1 (janv-mars)
# ============================================================

os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Referentiel centrales ---
PLANT_LOCATIONS = {
    "PACA": {"lat_range": (43.2, 44.0), "lon_range": (5.0, 7.0), "ghi_factor": 1.15},
    "Occitanie": {"lat_range": (42.5, 44.0), "lon_range": (1.5, 4.5), "ghi_factor": 1.10},
    "Auvergne-Rhone-Alpes": {"lat_range": (44.5, 46.0), "lon_range": (3.5, 6.5), "ghi_factor": 0.95},
    "Nouvelle-Aquitaine": {"lat_range": (44.0, 46.5), "lon_range": (-1.0, 1.5), "ghi_factor": 1.00},
    "Corse": {"lat_range": (41.5, 43.0), "lon_range": (8.5, 9.5), "ghi_factor": 1.20},
}

PANEL_MODELS = {
    "LONGi Hi-MO 5": {"pmax_w": 555, "efficiency": 0.213, "temp_coeff": -0.0034, "noct": 45},
    "Jinko Tiger Neo": {"pmax_w": 580, "efficiency": 0.221, "temp_coeff": -0.0029, "noct": 43},
    "Canadian Solar HiKu6": {"pmax_w": 540, "efficiency": 0.209, "temp_coeff": -0.0036, "noct": 45},
}

INVERTER_MODELS = {
    "Huawei SUN2000-100KTL": {"rated_kw": 100, "efficiency": 0.987, "mppt_channels": 10},
    "SMA Sunny Tripower": {"rated_kw": 110, "efficiency": 0.983, "mppt_channels": 6},
}

# Affecter regions aux centrales
plant_info = {}
regions = list(PLANT_LOCATIONS.keys())
for pid in range(1, NB_PLANTS + 1):
    region = regions[pid % len(regions)]
    loc = PLANT_LOCATIONS[region]
    panel_model = random.choice(list(PANEL_MODELS.keys()))
    inverter_model = random.choice(list(INVERTER_MODELS.keys()))
    plant_info[pid] = {
        "plant_id": f"PV-{pid:03d}",
        "plant_name": f"Centrale Solaire {region}-{pid:02d}",
        "region": region,
        "latitude": round(random.uniform(*loc["lat_range"]), 4),
        "longitude": round(random.uniform(*loc["lon_range"]), 4),
        "ghi_factor": loc["ghi_factor"],
        "panel_model": panel_model,
        "inverter_model": inverter_model,
        "tilt_deg": random.choice([20, 25, 30, 35]),
        "azimuth_deg": random.choice([170, 175, 180, 185, 190]),
        "install_year": random.randint(2016, 2023),
        "nb_strings": 100,
    }

# Referentiel chaines
string_info = {}
string_id = 0
# Defauts injectes
SOILED_STRINGS = set()       # Encrassement (correle au temps)
DEGRADED_STRINGS = set()     # Vieillissement premature
INVERTER_FAULT_STRINGS = set()  # Defaut onduleur
SENSOR_FAULT_STRINGS = set()   # Capteur defaillant

for pid in range(1, NB_PLANTS + 1):
    p = plant_info[pid]
    pspecs = PANEL_MODELS[p["panel_model"]]
    ispecs = INVERTER_MODELS[p["inverter_model"]]

    for s in range(1, p["nb_strings"] + 1):
        string_id += 1
        inverter_id = f"INV-{pid:03d}-{((s - 1) // ispecs['mppt_channels']) + 1:02d}"

        string_info[string_id] = {
            "string_id": f"STR-{string_id:05d}",
            "plant_id": p["plant_id"],
            "inverter_id": inverter_id,
            "panel_model": p["panel_model"],
            "nb_panels": PANELS_PER_STRING,
            "string_pmax_w": pspecs["pmax_w"] * PANELS_PER_STRING,
            "tilt_deg": p["tilt_deg"],
            "azimuth_deg": p["azimuth_deg"],
        }

# Injecter 5% de chaines defectueuses = 125 chaines
all_string_ids = list(range(1, NB_STRINGS + 1))
defect_ids = random.sample(all_string_ids, 125)
SOILED_STRINGS = set(defect_ids[:50])         # 50 chaines encrassees
DEGRADED_STRINGS = set(defect_ids[50:85])     # 35 chaines vieillies
INVERTER_FAULT_STRINGS = set(defect_ids[85:110])  # 25 chaines sur onduleur defaillant
SENSOR_FAULT_STRINGS = set(defect_ids[110:125])    # 15 chaines capteur defaillant

# Ecriture referentiel centrales
plant_path = os.path.join(OUTPUT_DIR, "plants.csv")
with open(plant_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["plant_id", "plant_name", "region", "latitude", "longitude",
                "panel_model", "inverter_model", "tilt_deg", "azimuth_deg",
                "install_year", "nb_strings"])
    for p in plant_info.values():
        w.writerow([p["plant_id"], p["plant_name"], p["region"],
                    p["latitude"], p["longitude"], p["panel_model"],
                    p["inverter_model"], p["tilt_deg"], p["azimuth_deg"],
                    p["install_year"], p["nb_strings"]])
print(f"Centrales: {plant_path} ({NB_PLANTS} centrales)")

# Ecriture referentiel chaines
string_path = os.path.join(OUTPUT_DIR, "strings.csv")
with open(string_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["string_id", "plant_id", "inverter_id", "panel_model",
                "nb_panels", "string_pmax_w", "tilt_deg", "azimuth_deg"])
    for s in string_info.values():
        w.writerow([s["string_id"], s["plant_id"], s["inverter_id"],
                    s["panel_model"], s["nb_panels"], s["string_pmax_w"],
                    s["tilt_deg"], s["azimuth_deg"]])
print(f"Chaines: {string_path} ({NB_STRINGS} chaines)")

# Nettoyage log
cleaning_path = os.path.join(OUTPUT_DIR, "cleaning_log.csv")
with open(cleaning_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["plant_id", "cleaning_date", "nb_strings_cleaned", "method"])
    for pid in range(1, NB_PLANTS + 1):
        # 1-2 nettoyages par an
        for _ in range(random.randint(1, 2)):
            dt = datetime(YEAR, random.randint(1, 12), random.randint(1, 28))
            nb = random.randint(20, 100)
            method = random.choice(["water_robot", "manual", "drone_spray"])
            w.writerow([f"PV-{pid:03d}", dt.strftime("%Y-%m-%d"), nb, method])
print(f"Nettoyage: {cleaning_path}")

# ---------------------------------------------------------------
# DONNEES DE PRODUCTION (mesures toutes les 15 min)
# ---------------------------------------------------------------
def solar_irradiance(hour, month, ghi_factor):
    """Irradiance solaire simulee (W/m2)."""
    if hour < 7 or hour > 20:
        return 0
    # Courbe solaire en cloche
    solar_noon = 12.5
    max_irr_base = 400 + 200 * ((month - 1) / 11)  # hiver=400, ete=600
    sigma = 3.5
    irr = max_irr_base * math.exp(-0.5 * ((hour - solar_noon) / sigma) ** 2)
    irr *= ghi_factor
    irr += random.gauss(0, 30)
    return max(0, irr)

def panel_temp(irradiance, ambient_temp, noct):
    """Temperature panneau selon modele NOCT."""
    if irradiance <= 0:
        return ambient_temp
    return ambient_temp + (noct - 20) * (irradiance / 800)

def theoretical_dc_power(irradiance, cell_temp, pmax, temp_coeff):
    """Puissance DC theorique selon STC + correction temperature."""
    if irradiance <= 10:
        return 0
    # P_theo = Pmax * (G / 1000) * (1 + temp_coeff * (Tcell - 25))
    power = pmax * (irradiance / 1000) * (1 + temp_coeff * (cell_temp - 25))
    return max(0, power)

print("\nGeneration des donnees de production...")
prod_path = os.path.join(OUTPUT_DIR, "production_data.csv")
total_rows = 0

with open(prod_path, "w", newline="", encoding="utf-8") as f:
    w = csv.writer(f)
    w.writerow(["timestamp", "string_id", "plant_id", "inverter_id",
                "irradiance_wm2", "ambient_temp_c", "panel_temp_c",
                "dc_power_w", "ac_power_w", "voltage_v", "current_a",
                "status"])

    start_dt = datetime(YEAR, 1, 1, 6, 0)
    end_dt = datetime(YEAR, 1, 1 + SIMULATION_DAYS - 1, 20, 45) if SIMULATION_DAYS < 28 \
        else datetime(YEAR, 3, 31, 20, 45)
    current = start_dt

    while current <= end_dt:
        hour = current.hour + current.minute / 60
        month = current.month
        dow = current.weekday()

        # Sauter les heures de nuit (pas de production)
        if hour < 6 or hour > 21:
            current += timedelta(minutes=INTERVAL_MINUTES)
            continue

        for sid in range(1, NB_STRINGS + 1):
            s = string_info[sid]
            pid = int(s["plant_id"].split("-")[1])
            p = plant_info[pid]
            pspecs = PANEL_MODELS[s["panel_model"]]
            ispecs = INVERTER_MODELS[p["inverter_model"]]

            irr = solar_irradiance(hour, month, p["ghi_factor"])
            ambient = 5 + 10 * ((month - 1) / 11) + random.gauss(0, 3)
            p_temp = panel_temp(irr, ambient, pspecs["noct"])

            # Puissance theorique pour la chaine
            theo_dc = theoretical_dc_power(irr, p_temp,
                        pspecs["pmax_w"] * s["nb_panels"],
                        pspecs["temp_coeff"])

            # Rendement normal 90-98%
            eff = random.uniform(0.90, 0.98)
            status = "running"

            # --- INJECTION DES DEFAUTS ---
            if sid in SOILED_STRINGS:
                # Encrassement progressif : pire en fin de trimestre
                day_of_sim = (current - datetime(YEAR, 1, 1)).days
                soil_factor = max(0.55, 1.0 - day_of_sim * 0.004)
                eff *= soil_factor

            elif sid in DEGRADED_STRINGS:
                # Vieillissement : ratio stable a 65-75%
                eff = random.uniform(0.60, 0.75)

            elif sid in INVERTER_FAULT_STRINGS:
                # Defaut onduleur : baisse brutale synchrone
                if random.random() < 0.15:
                    eff = random.uniform(0.20, 0.50)
                    status = "mppt_fault"
                else:
                    eff = random.uniform(0.80, 0.90)

            elif sid in SENSOR_FAULT_STRINGS:
                # Capteur defaillant : lectures aberrantes
                if random.random() < 0.2:
                    irr = irr * random.uniform(0.1, 3.0)
                    status = "sensor_warning"

            dc_power = theo_dc * eff + random.gauss(0, 50)
            dc_power = max(0, dc_power)
            ac_power = dc_power * ispecs["efficiency"] * random.uniform(0.97, 1.0)
            ac_power = max(0, ac_power)

            # Tension et courant
            voltage = 600 + random.gauss(0, 20) if irr > 50 else 0
            current_a_val = dc_power / max(voltage, 1) if voltage > 0 else 0

            if irr < 20:
                status = "low_irradiance"
                dc_power = 0
                ac_power = 0

            w.writerow([
                current.isoformat(),
                s["string_id"],
                s["plant_id"],
                s["inverter_id"],
                round(irr, 1),
                round(ambient, 1),
                round(p_temp, 1),
                round(dc_power, 2),
                round(ac_power, 2),
                round(max(0, voltage), 1),
                round(max(0, current_a_val), 3),
                status
            ])
            total_rows += 1

        current += timedelta(minutes=INTERVAL_MINUTES)
        if total_rows % 1_000_000 == 0:
            print(f"  {total_rows:,} lignes...")

print(f"\nProduction: {prod_path}")
print(f"Total: {total_rows:,} lignes")
print(f"\nDefauts injectes:")
print(f"  Chaines encrassees:        {len(SOILED_STRINGS)} (degradation progressive)")
print(f"  Chaines vieillies:         {len(DEGRADED_STRINGS)} (ratio 65-75% constant)")
print(f"  Defaut onduleur (MPPT):    {len(INVERTER_FAULT_STRINGS)} (baisses brutales)")
print(f"  Capteurs defaillants:      {len(SENSOR_FAULT_STRINGS)} (lectures aberrantes)")
print(f"  Total panneaux touches: {125 * PANELS_PER_STRING:,} / {NB_STRINGS * PANELS_PER_STRING:,}")
```

### 1.2 ‚Äî Lancer la g√©n√©ration

```bash
py generate_solar_park_data.py
```

**Dur√©e** : 10-20 minutes. **Fichiers produits** :

| Fichier               | Taille estim√©e | Lignes      |
| --------------------- | -------------- | ----------- |
| `production_data.csv` | ~1.5 Go        | ~15 000 000 |
| `plants.csv`          | ~2 Ko          | 25          |
| `strings.csv`         | ~150 Ko        | 2 500       |
| `cleaning_log.csv`    | ~2 Ko          | ~40         |

> üí° **Astuce test rapide** : R√©duisez `SIMULATION_DAYS = 7` pour g√©n√©rer ~1.2M de lignes au lieu de 15M.

---

## Partie 2 ‚Äî Lakehouse : Ingestion Bronze

### 2.1 ‚Äî Cr√©er le Lakehouse

1. **app.fabric.microsoft.com** ‚Üí votre workspace
2. **+ New item** ‚Üí **Lakehouse**
3. Nom : **`LH_SolarPark`**
4. Cliquer **Create**

### 2.2 ‚Äî Charger les fichiers

1. **Files** ‚Üí cr√©er sous-dossier **`bronze`**
2. Dans **bronze** : cr√©er **`production`**, **`plants`**, **`strings`**, **`cleaning`**
3. Charger chaque CSV dans son dossier

> ‚ö†Ô∏è **Pi√®ge** : Pour `production_data.csv` (1.5 Go), utilisez **OneLake File Explorer** (glisser-d√©poser). L'upload navigateur peut √©chouer sur les gros fichiers.

---

## Partie 3 ‚Äî Phase 1 : Exploration (Bronze ‚Üí Silver simple) avec Spark SQL

### 3.1 ‚Äî Cr√©er le Notebook

1. Workspace ‚Üí **+ New item** ‚Üí **Notebook**
2. Nommer : **`NB_Solar_Performance`**
3. Attacher au Lakehouse **LH_SolarPark**

### 3.2 ‚Äî Cellule 1 : Chargement Bronze

```python
# === CELLULE 1 : Chargement Bronze ===

df_prod = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/production/production_data.csv")
)
df_prod.write.mode("overwrite").format("delta").saveAsTable("bronze_production")

df_plants = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/plants/plants.csv")
)
df_plants.write.mode("overwrite").format("delta").saveAsTable("bronze_plants")

df_strings = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/strings/strings.csv")
)
df_strings.write.mode("overwrite").format("delta").saveAsTable("bronze_strings")

df_cleaning = (spark.read
    .option("header", True)
    .option("inferSchema", True)
    .csv("Files/bronze/cleaning/cleaning_log.csv")
)
df_cleaning.write.mode("overwrite").format("delta").saveAsTable("bronze_cleaning")

print(f"Production : {df_prod.count():,} lignes")
print(f"Centrales  : {df_plants.count()}")
print(f"Chaines    : {df_strings.count():,}")
print(f"Nettoyage  : {df_cleaning.count()}")
```

### 3.3 ‚Äî Cellule 2 : Qualit√© des donn√©es et nettoyage

**Probl√®me terrain** : Les donn√©es brutes sont h√©t√©rog√®nes ‚Äî lectures nocturnes, capteurs d√©faillants, valeurs absurdes. Il faut nettoyer avant d'analyser.

```sql
%%sql
-- === CELLULE 2 : Diagnostic qualite des donnees ===

SELECT
    status,
    COUNT(*) AS nb_readings,
    ROUND(AVG(irradiance_wm2), 1) AS avg_irradiance,
    ROUND(AVG(dc_power_w), 1) AS avg_dc_power,
    ROUND(MIN(dc_power_w), 1) AS min_dc_power,
    ROUND(MAX(dc_power_w), 1) AS max_dc_power,
    SUM(CASE WHEN irradiance_wm2 < 0 THEN 1 ELSE 0 END) AS negative_irradiance,
    SUM(CASE WHEN dc_power_w < 0 THEN 1 ELSE 0 END) AS negative_power
FROM bronze_production
GROUP BY status
ORDER BY nb_readings DESC
```

**Interpr√©tation** : Les lignes avec `status = low_irradiance` sont normales (nuit/aube/cr√©puscule). `sensor_warning` et `mppt_fault` sont les anomalies √† investiguer. Les valeurs n√©gatives sont des artefacts de capteurs ‚Üí √† filtrer.

### 3.4 ‚Äî Cellule 3 : Vue d'ensemble par centrale

**Question m√©tier** : Quelles centrales produisent le plus et le moins par rapport √† leur capacit√© install√©e ?

```sql
%%sql
-- === CELLULE 3 : Performance par centrale ===

SELECT
    p.plant_id,
    p.region,
    p.panel_model,
    p.nb_strings,
    COUNT(*) AS nb_readings,
    ROUND(AVG(pr.dc_power_w), 1) AS avg_dc_power_w,
    ROUND(AVG(pr.ac_power_w), 1) AS avg_ac_power_w,
    ROUND(SUM(pr.ac_power_w * 0.25 / 1000) / 1000, 1) AS total_production_mwh,
    ROUND(AVG(pr.irradiance_wm2), 1) AS avg_irradiance,
    ROUND(
        SUM(pr.ac_power_w * 0.25) /
        NULLIF(SUM(pr.irradiance_wm2 * 0.25 * p.nb_strings * 48 * 2.0 / 1000), 0)
    * 100, 1) AS approx_pr_pct
FROM bronze_production pr
JOIN bronze_plants p ON pr.plant_id = p.plant_id
WHERE pr.status = 'running' AND pr.irradiance_wm2 > 50
GROUP BY p.plant_id, p.region, p.panel_model, p.nb_strings
ORDER BY approx_pr_pct ASC
```

**Interpr√©tation** : `approx_pr_pct` est un ratio de performance approximatif. Les centrales en bas du classement m√©ritent une investigation par cha√Æne. Si une centrale enti√®re sous-performe ‚Üí le probl√®me est syst√©mique (onduleur central, ombrage site). Si seulement quelques cha√Ænes ‚Üí le probl√®me est local (panneaux sales, d√©grad√©s).

---

## Partie 4 ‚Äî Phase 2 : Investigation approfondie (Silver avanc√©) avec PySpark

### 4.1 ‚Äî Cellule 4 : UDF ‚Äî Puissance DC th√©orique photovolta√Øque

**Probl√®me terrain** : La puissance th√©orique d√©pend de l'irradiance, de la temp√©rature du panneau et du coefficient de temp√©rature ‚Äî une relation non lin√©aire qui n√©cessite une UDF.

**Question m√©tier** : Pour chaque mesure, quelle est la production th√©orique attendue par le constructeur ?

```python
# === CELLULE 4 : UDF puissance DC theorique PV ===

from pyspark.sql import functions as F
from pyspark.sql.types import DoubleType, StructType, StructField, StringType

# Charger les specs techniques
df_strings = spark.table("bronze_strings")
df_plants = spark.table("bronze_plants")

# Construire la map de specs par string_id
string_specs = (df_strings
    .select("string_id", "string_pmax_w", "panel_model", "nb_panels")
    .collect()
)

# Specs panneaux
PANEL_SPECS = {
    "LONGi Hi-MO 5": {"pmax_w": 555, "temp_coeff": -0.0034, "noct": 45},
    "Jinko Tiger Neo": {"pmax_w": 580, "temp_coeff": -0.0029, "noct": 43},
    "Canadian Solar HiKu6": {"pmax_w": 540, "temp_coeff": -0.0036, "noct": 45},
}

spec_map = {}
for row in string_specs:
    ps = PANEL_SPECS.get(row.panel_model, {})
    spec_map[row.string_id] = {
        "string_pmax_w": row.string_pmax_w,
        "temp_coeff": ps.get("temp_coeff", -0.0035),
        "nb_panels": row.nb_panels,
    }

spec_broadcast = spark.sparkContext.broadcast(spec_map)

@F.udf(DoubleType())
def calc_theoretical_dc(string_id, irradiance, panel_temp):
    """Puissance DC theorique : Pmax * (G/1000) * (1 + Tc * (Tcell - 25))"""
    specs = spec_broadcast.value.get(string_id)
    if specs is None or irradiance is None or panel_temp is None:
        return 0.0
    if irradiance < 10:
        return 0.0
    pmax = float(specs["string_pmax_w"])
    tc = float(specs["temp_coeff"])
    power = pmax * (irradiance / 1000.0) * (1.0 + tc * (panel_temp - 25.0))
    return max(0.0, power)


# Appliquer sur les donnees de production
df_prod = spark.table("bronze_production")

df_silver = (df_prod
    .filter(F.col("status").isin(["running", "mppt_fault"]))
    .filter(F.col("irradiance_wm2") > 50)
    .withColumn("theoretical_dc_w",
                calc_theoretical_dc(F.col("string_id"),
                                    F.col("irradiance_wm2"),
                                    F.col("panel_temp_c")))
    .withColumn("performance_ratio",
                F.when(F.col("theoretical_dc_w") > 100,
                       F.round(F.col("dc_power_w") / F.col("theoretical_dc_w"), 4))
                .otherwise(None))
    .withColumn("reading_date", F.to_date("timestamp"))
    .withColumn("reading_hour", F.hour("timestamp"))
    .withColumn("reading_month", F.month("timestamp"))
    .withColumn("day_of_sim",
                F.datediff(F.col("reading_date"), F.lit("2025-01-01")))
)

df_silver.write.mode("overwrite").format("delta").saveAsTable("silver_production")
print(f"silver_production: {df_silver.count():,} lignes")
```

**Interpr√©tation** : Chaque ligne a maintenant un `performance_ratio`. La formule `Pmax √ó (G/1000) √ó (1 + Tc √ó (Tcell - 25))` est le standard IEC 61724. Un PR de 0.90 = la cha√Æne produit 90% du th√©orique ‚Üí normal. Un PR de 0.65 = elle perd 35% ‚Üí investigation urgente. Le `day_of_sim` servira √† d√©tecter la d√©gradation progressive (encrassement).

### 4.2 ‚Äî Cellule 5 : Profil par cha√Æne et d√©tection des sous-performantes

```python
# === CELLULE 5 : Profil par chaine ===

df_string_profile = spark.sql("""
    SELECT
        string_id,
        plant_id,
        inverter_id,
        COUNT(*) AS nb_readings,
        ROUND(AVG(performance_ratio), 4) AS avg_pr,
        ROUND(percentile_approx(performance_ratio, 0.1), 4) AS p10_pr,
        ROUND(percentile_approx(performance_ratio, 0.5), 4) AS p50_pr,
        ROUND(STDDEV(performance_ratio), 4) AS stddev_pr,
        ROUND(AVG(dc_power_w), 1) AS avg_dc_power,
        ROUND(AVG(theoretical_dc_w), 1) AS avg_theoretical_dc,
        ROUND(SUM(theoretical_dc_w - dc_power_w) * 0.25 / 1000000, 2) AS lost_energy_mwh,
        SUM(CASE WHEN status = 'mppt_fault' THEN 1 ELSE 0 END) AS mppt_faults,
        -- Correlation avec le temps (encrassement)
        ROUND(corr(day_of_sim, performance_ratio), 4) AS time_pr_correlation,
        -- PR premiere semaine vs derniere semaine
        ROUND(AVG(CASE WHEN day_of_sim <= 7 THEN performance_ratio END), 4) AS pr_first_week,
        ROUND(AVG(CASE WHEN day_of_sim >= 83 THEN performance_ratio END), 4) AS pr_last_week
    FROM silver_production
    WHERE theoretical_dc_w > 500
    GROUP BY string_id, plant_id, inverter_id
""")

df_string_profile.write.mode("overwrite").format("delta").saveAsTable("silver_string_profiles")

# Chaines sous-performantes (PR < 0.75)
underperformers = df_string_profile.filter(F.col("avg_pr") < 0.75).count()
print(f"Chaines avec PR < 75%: {underperformers}")
print(f"(attendu: ~125 chaines defectueuses)")
```

**Interpr√©tation** : `time_pr_correlation` est la cl√© du diagnostic. Une corr√©lation fortement n√©gative (< -0.5) signifie que le PR se d√©grade avec le temps ‚Üí encrassement. Une corr√©lation proche de z√©ro avec un PR bas constant ‚Üí vieillissement. `mppt_faults > 0` ‚Üí probl√®me onduleur. `pr_first_week` vs `pr_last_week` quantifie la d√©rive : si la cha√Æne passe de 0.92 √† 0.70 en 3 mois, c'est de l'encrassement acc√©l√©r√©.

### 4.3 ‚Äî Cellule 6 : KMeans ‚Äî Clustering des types de d√©fauts

**Probl√®me terrain** : L'√©quipe maintenance a besoin de savoir **quel type** de d√©faut affecter √† chaque cha√Æne pour envoyer la bonne √©quipe (nettoyage, remplacement, recalibrage onduleur).

```python
# === CELLULE 6 : KMeans clustering des defauts (SparkML natif) ===

from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline

# Preparer les features
df_profiles = spark.table("silver_string_profiles").filter(F.col("avg_pr") < 0.85)

feature_cols = [
    "avg_pr", "stddev_pr", "time_pr_correlation",
    "mppt_faults", "lost_energy_mwh"
]

for col_name in feature_cols:
    df_profiles = df_profiles.withColumn(col_name,
        F.coalesce(F.col(col_name), F.lit(0.0)))

# Pipeline SparkML
assembler = VectorAssembler(inputCols=feature_cols, outputCol="raw_features")
scaler = StandardScaler(inputCol="raw_features", outputCol="features",
                        withStd=True, withMean=True)
kmeans = KMeans(featuresCol="features", predictionCol="cluster",
                k=4, seed=42, maxIter=50)

pipeline = Pipeline(stages=[assembler, scaler, kmeans])
model = pipeline.fit(df_profiles)
df_clustered = model.transform(df_profiles)

# Analyser les clusters
cluster_summary = (df_clustered
    .groupBy("cluster")
    .agg(
        F.count("*").alias("nb_strings"),
        F.round(F.avg("avg_pr"), 3).alias("avg_pr"),
        F.round(F.avg("time_pr_correlation"), 3).alias("avg_time_corr"),
        F.round(F.avg("mppt_faults"), 1).alias("avg_mppt_faults"),
        F.round(F.avg("stddev_pr"), 4).alias("avg_stddev_pr"),
    )
)
cluster_summary.show(truncate=False)

# Labelliser les clusters
df_labelled = (df_clustered
    .withColumn("defect_type",
        F.when((F.col("time_pr_correlation") < -0.3) & (F.col("mppt_faults") < 5),
               "ENCRASSEMENT - Nettoyage prioritaire")
        .when((F.col("stddev_pr") < 0.05) & (F.col("avg_pr") < 0.78),
               "VIEILLISSEMENT - Remplacement panneaux")
        .when(F.col("mppt_faults") > 5,
               "DEFAUT_ONDULEUR - Recalibrage MPPT")
        .when(F.col("stddev_pr") > 0.15,
               "CAPTEUR_DEFAILLANT - Verification instrumentation")
        .otherwise("SOUS_PERFORMANCE - Investigation"))
)

df_labelled.select("string_id", "plant_id", "inverter_id", "avg_pr",
                   "time_pr_correlation", "mppt_faults", "defect_type"
).write.mode("overwrite").format("delta").saveAsTable("silver_defect_classification")

# Resume
(df_labelled.groupBy("defect_type").agg(
    F.count("*").alias("nb_chaines"),
    F.round(F.avg("avg_pr"), 3).alias("avg_pr"),
    F.round(F.sum("lost_energy_mwh"), 1).alias("total_lost_mwh")
).orderBy("avg_pr").show(truncate=False))
```

**Interpr√©tation** : Le KMeans s√©pare automatiquement les panneaux en groupes homog√®nes. La labellisation post-clustering utilise les caract√©ristiques physiques pour nommer chaque groupe. `ENCRASSEMENT` = PR qui baisse avec le temps ‚Üí envoyer l'√©quipe de nettoyage robotis√©. `VIEILLISSEMENT` = PR stable mais bas ‚Üí programmer le remplacement lors de la prochaine maintenance. `DEFAUT_ONDULEUR` = faults MPPT fr√©quents ‚Üí recalibrer l'onduleur (15 min de travail, gain imm√©diat). `CAPTEUR` = lectures erratiques ‚Üí ne pas agir sur les panneaux, corriger le capteur d'abord.

---

## Partie 5 ‚Äî Phase 3 : Synth√®se actionnable (Gold)

### 5.1 ‚Äî Cellule 7 : Tables Gold

```python
# === CELLULE 7 : Tables Gold ===

# --- Gold 1 : Plan d'intervention par type de defaut ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_maintenance_plan AS
    SELECT
        defect_type,
        COUNT(*) AS nb_chaines,
        COUNT(*) * 48 AS nb_panneaux_touches,
        COLLECT_SET(plant_id) AS centrales_concernees,
        ROUND(AVG(avg_pr) * 100, 1) AS avg_pr_pct,
        ROUND(SUM(lost_energy_mwh), 1) AS total_lost_mwh,
        ROUND(SUM(lost_energy_mwh) * 75, 0) AS lost_revenue_eur,
        CASE
            WHEN defect_type LIKE '%ENCRASSEMENT%' THEN 'Nettoyage robot/drone'
            WHEN defect_type LIKE '%VIEILLISSEMENT%' THEN 'Remplacement panneaux'
            WHEN defect_type LIKE '%ONDULEUR%' THEN 'Recalibrage MPPT'
            WHEN defect_type LIKE '%CAPTEUR%' THEN 'Remplacement capteur'
            ELSE 'Investigation terrain'
        END AS action,
        CASE
            WHEN defect_type LIKE '%ENCRASSEMENT%' THEN ROUND(COUNT(*) * 15, 0)
            WHEN defect_type LIKE '%VIEILLISSEMENT%' THEN ROUND(COUNT(*) * 48 * 200, 0)
            WHEN defect_type LIKE '%ONDULEUR%' THEN ROUND(COUNT(DISTINCT inverter_id) * 500, 0)
            WHEN defect_type LIKE '%CAPTEUR%' THEN ROUND(COUNT(*) * 300, 0)
            ELSE 0
        END AS estimated_fix_cost_eur,
        CASE
            WHEN defect_type LIKE '%ONDULEUR%' THEN 1
            WHEN defect_type LIKE '%ENCRASSEMENT%' THEN 2
            WHEN defect_type LIKE '%CAPTEUR%' THEN 3
            WHEN defect_type LIKE '%VIEILLISSEMENT%' THEN 4
            ELSE 5
        END AS priority
    FROM silver_defect_classification
    GROUP BY defect_type
    ORDER BY priority
""")

# --- Gold 2 : KPI quotidiens par centrale ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_daily_plant_kpi AS
    SELECT
        reading_date,
        p.plant_id,
        p.region,
        COUNT(DISTINCT pr.string_id) AS active_strings,
        ROUND(AVG(pr.performance_ratio), 3) AS avg_pr,
        ROUND(SUM(pr.ac_power_w * 0.25 / 1000000), 3) AS production_mwh,
        ROUND(SUM(pr.theoretical_dc_w * 0.25 / 1000000), 3) AS theoretical_mwh,
        ROUND(AVG(pr.irradiance_wm2), 1) AS avg_irradiance,
        ROUND(AVG(pr.panel_temp_c), 1) AS avg_panel_temp,
        SUM(CASE WHEN pr.performance_ratio < 0.75 THEN 1 ELSE 0 END) AS low_pr_readings,
        SUM(CASE WHEN pr.status = 'mppt_fault' THEN 1 ELSE 0 END) AS mppt_faults
    FROM silver_production pr
    JOIN bronze_plants p ON pr.plant_id = p.plant_id
    WHERE pr.theoretical_dc_w > 500
    GROUP BY reading_date, p.plant_id, p.region
""")

# --- Gold 3 : Resume executif ---
spark.sql("""
    CREATE OR REPLACE TABLE gold_executive_summary AS
    SELECT
        defect_type AS category,
        nb_chaines,
        nb_panneaux_touches,
        total_lost_mwh,
        lost_revenue_eur,
        estimated_fix_cost_eur,
        CASE
            WHEN estimated_fix_cost_eur > 0
            THEN ROUND(lost_revenue_eur / estimated_fix_cost_eur * 100, 0)
            ELSE 0
        END AS roi_pct,
        action,
        priority
    FROM gold_maintenance_plan
    ORDER BY priority
""")

print("=== PLAN DE MAINTENANCE ===")
spark.sql("SELECT * FROM gold_maintenance_plan").show(truncate=False)

print("\n=== RESUME EXECUTIF ===")
spark.sql("SELECT category, nb_panneaux_touches, lost_revenue_eur, estimated_fix_cost_eur, roi_pct, action FROM gold_executive_summary ORDER BY priority").show(truncate=False)
```

**Interpr√©tation du r√©sultat Gold** :

Le tableau `gold_maintenance_plan` est **le plan d'action du directeur d'exploitation** :

| D√©faut              | Nb panneaux | Ce qu'on fait                                             | Co√ªt      | Gain/an         | ROI    |
| ------------------- | ----------- | --------------------------------------------------------- | --------- | --------------- | ------ |
| **DEFAUT_ONDULEUR** | ~1 200      | Recalibrer les MPPT (15 min par onduleur)                 | ~5 000 ‚Ç¨  | ~50 000 ‚Ç¨       | 1000%  |
| **ENCRASSEMENT**    | ~2 400      | Envoyer robots/drones de nettoyage sur 4 centrales        | ~750 ‚Ç¨    | ~280 000 ‚Ç¨      | >1000% |
| **CAPTEUR**         | ~720        | Remplacer les capteurs d√©faillants                        | ~4 500 ‚Ç¨  | Qualit√© donn√©es | -      |
| **VIEILLISSEMENT**  | ~1 680      | Remplacer les panneaux d√©grad√©s (prochain arr√™t planifi√©) | ~70 000 ‚Ç¨ | ~150 000 ‚Ç¨      | 214%   |

Le recalibrage onduleur a le meilleur ROI (15 min de travail pour des milliers d'euros r√©cup√©r√©s). Le nettoyage est le plus gros gisement en volume. Le remplacement de panneaux est un investissement plus lourd mais rentable en 6 mois.

**Total : ~480 000 ‚Ç¨/an r√©cup√©rables. ROI global de l'analyse : > 1000%.**

---

## Partie 6 ‚Äî V√©rification finale

### 6.1 ‚Äî Cellule 8 : Validation crois√©e

```python
# === CELLULE 8 : Validation ===

print("=== DEFAUTS INJECTES ===")
print("Chaines encrassees:     50 (degradation progressive)")
print("Chaines vieillies:      35 (ratio 65-75% constant)")
print("Defaut onduleur MPPT:   25 (baisses brutales)")
print("Capteurs defaillants:   15 (lectures aberrantes)")
print("Total:                 125 chaines / 6 000 panneaux")
print()

df_defects = spark.table("silver_defect_classification")
print(f"Chaines defectueuses detectees: {df_defects.count()}")
print()
df_defects.groupBy("defect_type").count().orderBy(F.desc("count")).show(truncate=False)
```

---

## R√©capitulatif : outils utilis√©s et justification

| √âtape                                      | Outil                                 | Pourquoi cet outil                          |
| ------------------------------------------ | ------------------------------------- | ------------------------------------------- |
| Chargement CSV ‚Üí Delta                     | PySpark                               | Volume important (~15M lignes)              |
| Qualit√© donn√©es, stats par centrale        | **Spark SQL** (`%%sql`)               | Agr√©gations, GROUP BY ‚Äî lisible             |
| UDF puissance DC th√©orique (IEC 61724)     | **PySpark UDF** + broadcast           | Formule non lin√©aire avec lookup specs      |
| Performance ratio par mesure               | **PySpark**                           | Colonne calcul√©e via UDF                    |
| Profil par cha√Æne + corr√©lation temporelle | **Spark SQL** via `spark.sql()`       | `percentile_approx`, `corr()` disponibles   |
| KMeans clustering des d√©fauts              | **SparkML** (`pyspark.ml.clustering`) | S√©paration automatique en groupes homog√®nes |
| Labellisation post-clustering              | **PySpark**                           | Logique m√©tier conditionnelle (WHEN/THEN)   |
| Tables Gold et plan maintenance            | **Spark SQL**                         | `COLLECT_SET`, `CASE WHEN`, agr√©gations     |

**R√®gle d'or confirm√©e une troisi√®me fois** : Spark SQL pour l'exploration et les agr√©gats, PySpark d√®s qu'il y a une UDF physique (courbe PV), du ML (KMeans), ou des broadcast de r√©f√©rentiels.

---

## Conformit√© Fabric Runtime 1.3 ‚Äî Points v√©rifi√©s

| Point                                         | Statut | D√©tail                           |
| --------------------------------------------- | ------ | -------------------------------- |
| `%%sql` en t√™te des cellules SQL              | ‚úÖ      | Cellules 2 et 3                  |
| `percentile_approx()` au lieu de `PERCENTILE` | ‚úÖ      | Cellule 5                        |
| `pyspark.ml.clustering.KMeans` natif          | ‚úÖ      | Spark 3.5, pas besoin d'install  |
| `pyspark.ml.feature.StandardScaler`           | ‚úÖ      | Natif Runtime 1.3                |
| `@F.udf(DoubleType())` classique              | ‚úÖ      | API PySpark standard             |
| `broadcast()` sur SparkContext                | ‚úÖ      | Optimisation distribu√©e standard |
| `saveAsTable()` pour √©crire en Tables         | ‚úÖ      | Pas de `save("Tables/...")`      |
| Chemins relatifs `Files/bronze/...`           | ‚úÖ      | Lakehouse par d√©faut             |
| `COLLECT_SET()` en Spark SQL                  | ‚úÖ      | Support√© dans Spark 3.5          |
| `corr()` en Spark SQL                         | ‚úÖ      | Fonction d'agr√©gation native     |
| Delta Lake V-Order automatique                | ‚úÖ      | Activ√© par d√©faut dans Fabric    |
