{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 : Bronze - Import et première vue (Spark SQL)\n",
    "\n",
    "**Durée** : 10 minutes  \n",
    "**Lakehouse** : Lakehouse_bronze  \n",
    "**Objectif** : Charger les données brutes et identifier les problèmes de qualité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 1 : Configuration et vérification Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que Spark est disponible\n",
    "print(f\"✅ Spark version: {spark.version}\")\n",
    "print(f\"✅ Lakehouse par défaut: Lakehouse_bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 2 : Import consumption_raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le CSV avec PySpark\n",
    "df_consumption = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"Files/data_bronze_notebooks/consumption_raw.csv\")\n",
    "\n",
    "# Créer la table dans le schéma bronze\n",
    "df_consumption.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze.consumption_raw\")\n",
    "\n",
    "print(f\"✅ Table bronze.consumption_raw créée : {df_consumption.count()} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 3 : Première vue des données (voir le bordel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM bronze.consumption_raw LIMIT 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 4 : Détection des problèmes de qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Compter les NULL, codes erreur, doublons\n",
    "SELECT \n",
    "    COUNT(*) as total_rows,\n",
    "    COUNT(DISTINCT timestamp, site_id) as unique_rows,\n",
    "    COUNT(*) - COUNT(DISTINCT timestamp, site_id) as duplicates,\n",
    "    SUM(CASE WHEN consumption_mw IS NULL THEN 1 ELSE 0 END) as null_values,\n",
    "    SUM(CASE WHEN consumption_mw < 0 THEN 1 ELSE 0 END) as error_codes,\n",
    "    SUM(CASE WHEN consumption_mw > 10 THEN 1 ELSE 0 END) as outliers\n",
    "FROM bronze.consumption_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 5 : Import des autres fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market prices\n",
    "spark.read.csv(\"Files/data_bronze_notebooks/market_prices.csv\", header=True, inferSchema=True) \\\n",
    "    .write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze.market_prices\")\n",
    "\n",
    "# Weather\n",
    "spark.read.csv(\"Files/data_bronze_notebooks/weather_data.csv\", header=True, inferSchema=True) \\\n",
    "    .write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze.weather_data\")\n",
    "\n",
    "# Sites reference\n",
    "spark.read.csv(\"Files/data_bronze_notebooks/sites_reference.csv\", header=True, inferSchema=True) \\\n",
    "    .write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze.sites_reference\")\n",
    "\n",
    "# Maintenance events\n",
    "spark.read.csv(\"Files/data_bronze_notebooks/maintenance_events.csv\", header=True, inferSchema=True) \\\n",
    "    .write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze.maintenance_events\")\n",
    "\n",
    "print(\"✅ Toutes les tables Bronze créées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 6 : Vérification rapide de toutes les tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT 'consumption' as table_name, COUNT(*) as rows FROM bronze.consumption_raw\n",
    "UNION ALL\n",
    "SELECT 'prices', COUNT(*) FROM bronze.market_prices\n",
    "UNION ALL\n",
    "SELECT 'weather', COUNT(*) FROM bronze.weather_data\n",
    "UNION ALL\n",
    "SELECT 'sites', COUNT(*) FROM bronze.sites_reference\n",
    "UNION ALL\n",
    "SELECT 'maintenance', COUNT(*) FROM bronze.maintenance_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 7 : Résumé\n",
    "\n",
    "### ✅ Bronze import terminé\n",
    "\n",
    "**Données chargées** :\n",
    "- ~18,144 lignes de consommation (dont ~900 doublons, ~540 NULL, ~360 codes erreur)\n",
    "- 2,880 lignes de prix spot\n",
    "- 720 lignes météo\n",
    "- 6 sites\n",
    "- 8 événements maintenance\n",
    "\n",
    "**Problèmes détectés** :\n",
    "- ❌ 5% doublons\n",
    "- ❌ 3% valeurs NULL\n",
    "- ❌ 2% codes erreur (-999, -888, -777)\n",
    "- ❌ Formats de dates multiples\n",
    "- ❌ Valeurs aberrantes (> 10 MW)\n",
    "\n",
    "➡️ **Prochaine étape** : Nettoyage dans Silver (Notebook 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
