{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 : Silver 3 - ML Pr√©diction (PySpark MLlib)\n",
    "\n",
    "**Dur√©e** : 20 minutes  \n",
    "**Lakehouse** : Lakehouse_silver  \n",
    "**Objectif** : Entra√Æner un mod√®le de pr√©diction de consommation J+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 1 : Import MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"‚úÖ MLlib import√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 2 : Chargement donn√©es Silver enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"Lakehouse_silver.silver.consumption_enriched\")\n",
    "print(f\"üìä Donn√©es charg√©es : {df.count()} lignes\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 3 : Pr√©paration features pour ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features pertinentes\n",
    "features_cols = [\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"temperature_c\",\n",
    "    \"wind_speed_ms\",\n",
    "    \"consumption_lag_1d\",\n",
    "    \"consumption_lag_7d\",\n",
    "    \"baseline_7d_mw\",\n",
    "    \"price_eur_mwh\"\n",
    "]\n",
    "\n",
    "target_col = \"avg_consumption_mw\"\n",
    "\n",
    "# Filtrer les NULL (apr√®s lags)\n",
    "df_ml = df.select(features_cols + [target_col, \"hour\", \"site_id\"]).na.drop()\n",
    "\n",
    "print(f\"üìä Dataset ML : {df_ml.count()} lignes (apr√®s suppression NULL)\")\n",
    "df_ml.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 4 : VectorAssembler + StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=features_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ VectorAssembler + Scaler configur√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 5 : Split Train/Test (80/20 temporel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split temporel : 80% premiers jours = train, 20% derniers jours = test\n",
    "dates_sorted = df_ml.select(\"hour\").distinct().orderBy(\"hour\").collect()\n",
    "split_idx = int(len(dates_sorted) * 0.8)\n",
    "split_date = dates_sorted[split_idx][\"hour\"]\n",
    "\n",
    "train_df = df_ml.filter(F.col(\"hour\") < split_date)\n",
    "test_df = df_ml.filter(F.col(\"hour\") >= split_date)\n",
    "\n",
    "print(f\"üìä Train : {train_df.count()} lignes (80%)\")\n",
    "print(f\"üìä Test  : {test_df.count()} lignes (20%)\")\n",
    "print(f\"üìÖ Date de split : {split_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 6 : Entra√Ænement LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "print(\"üöÄ Entra√Ænement du mod√®le...\")\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"‚úÖ Mod√®le entra√Æn√©\")\n",
    "\n",
    "# Coefficients\n",
    "lr_model = model.stages[-1]\n",
    "print(f\"\\nüìä Intercept : {lr_model.intercept:.4f}\")\n",
    "print(f\"üìä Nombre de coefficients : {len(lr_model.coefficients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 7 : Pr√©dictions sur test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(\n",
    "    \"hour\", \n",
    "    \"site_id\", \n",
    "    target_col, \n",
    "    \"prediction\"\n",
    ").orderBy(\"hour\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 8 : √âvaluation (RMSE, MAE, R¬≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä M√âTRIQUES DU MOD√àLE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE (Root Mean Square Error) : {rmse:.4f} MW\")\n",
    "print(f\"MAE  (Mean Absolute Error)    : {mae:.4f} MW\")\n",
    "print(f\"R¬≤   (Coefficient determination): {r2:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if r2 > 0.7:\n",
    "    print(\"‚úÖ Mod√®le performant (R¬≤ > 0.7)\")\n",
    "elif r2 > 0.5:\n",
    "    print(\"‚ö†Ô∏è Mod√®le acceptable (R¬≤ entre 0.5 et 0.7)\")\n",
    "else:\n",
    "    print(\"‚ùå Mod√®le faible (R¬≤ < 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 9 : Analyse des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer erreur absolue\n",
    "predictions = predictions.withColumn(\n",
    "    \"error_abs\",\n",
    "    F.abs(F.col(target_col) - F.col(\"prediction\"))\n",
    ")\n",
    "\n",
    "error_stats = predictions.select(\n",
    "    F.mean(\"error_abs\").alias(\"mean_error\"),\n",
    "    F.stddev(\"error_abs\").alias(\"std_error\"),\n",
    "    F.max(\"error_abs\").alias(\"max_error\"),\n",
    "    F.min(\"error_abs\").alias(\"min_error\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nüìä STATISTIQUES DES ERREURS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Erreur moyenne    : {error_stats['mean_error']:.4f} MW\")\n",
    "print(f\"√âcart-type        : {error_stats['std_error']:.4f} MW\")\n",
    "print(f\"Erreur maximale   : {error_stats['max_error']:.4f} MW\")\n",
    "print(f\"Erreur minimale   : {error_stats['min_error']:.4f} MW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Top 10 pires pr√©dictions\n",
    "print(\"\\n‚ö†Ô∏è Top 10 pires pr√©dictions :\")\n",
    "predictions.select(\n",
    "    \"hour\", \n",
    "    \"site_id\", \n",
    "    target_col, \n",
    "    \"prediction\", \n",
    "    \"error_abs\"\n",
    ").orderBy(F.desc(\"error_abs\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 10 : Comparaison avec baseline na√Øve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline na√Øve : pr√©diction = consommation J-1\n",
    "predictions_baseline = predictions.withColumn(\n",
    "    \"prediction_baseline\",\n",
    "    F.col(\"consumption_lag_1d\")\n",
    ")\n",
    "\n",
    "rmse_baseline = RegressionEvaluator(\n",
    "    labelCol=target_col,\n",
    "    predictionCol=\"prediction_baseline\",\n",
    "    metricName=\"rmse\"\n",
    ").evaluate(predictions_baseline)\n",
    "\n",
    "print(\"\\nüìä COMPARAISON MOD√àLE vs BASELINE NA√èVE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE Mod√®le ML      : {rmse:.4f} MW\")\n",
    "print(f\"RMSE Baseline (J-1) : {rmse_baseline:.4f} MW\")\n",
    "improvement = ((rmse_baseline - rmse) / rmse_baseline * 100)\n",
    "print(f\"Am√©lioration        : {improvement:.1f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if rmse < rmse_baseline:\n",
    "    print(\"‚úÖ Le mod√®le ML est meilleur que la baseline na√Øve\")\n",
    "else:\n",
    "    print(\"‚ùå Le mod√®le ML n'am√©liore pas la baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 11 : Sauvegarde pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder toutes les pr√©dictions (train + test)\n",
    "all_predictions = model.transform(df_ml)\n",
    "\n",
    "all_predictions.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver.consumption_predictions\")\n",
    "\n",
    "print(f\"‚úÖ Pr√©dictions sauvegard√©es : {all_predictions.count()} lignes\")\n",
    "print(\"üìç Table : silver.consumption_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellule 12 : R√©sum√©\n",
    "\n",
    "### ‚úÖ ML Pr√©diction termin√©\n",
    "\n",
    "**Mod√®le entra√Æn√©** :\n",
    "- Algorithme : Linear Regression\n",
    "- Features : 8 (heure, jour, m√©t√©o, lags, baseline, prix)\n",
    "- Train/Test : 80/20 (split temporel)\n",
    "\n",
    "**Performance** :\n",
    "- RMSE, MAE, R¬≤ affich√©s ci-dessus\n",
    "- Am√©lioration vs baseline na√Øve calcul√©e\n",
    "\n",
    "**Pr√©dictions sauvegard√©es** :\n",
    "- üìç Table : silver.consumption_predictions\n",
    "\n",
    "**üí° Pourquoi PySpark √©tait obligatoire ?**\n",
    "- ‚ùå Spark SQL ne peut pas faire de ML (pas de MLlib en SQL)\n",
    "- ‚úÖ PySpark seul permet VectorAssembler, LinearRegression, Pipeline\n",
    "- ‚úÖ PySpark permet √©valuation avec m√©triques (RMSE, MAE, R¬≤)\n",
    "\n",
    "‚û°Ô∏è **Prochaine √©tape** : Agr√©gations business dans Gold (Notebook 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
