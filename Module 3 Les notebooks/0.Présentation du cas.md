# Étude de Cas : Chasse aux Rendements Perdus dans un Parc Éolien

#### **Partie 1 : État des Lieux – Une Richesse de Données Inexploitée**

Vous êtes ingénieur performance pour un parc de **50 éoliennes**. Chaque machine génère un flux continu de données : production (kWh), vitesse du vent (m/s), températures, vibrations, toutes les 15 minutes. À cela s’ajoutent les plannings de maintenance et les fiches techniques détaillées (courbes de puissance, capacités nominales).

*Problème initial* : Cette masse de données – des millions de lignes – est stockée à l’état brut dans un lac de données. Elle est désordonnée, pleine de valeurs aberrantes, de codes d’erreur (`-999`) et de formats de date incompatibles. **La richesse est là, mais elle est inexploitable.** Vous savez que le parc sous-performe par rapport aux prévisions, mais vous ne pouvez pas le prouver ni en identifier les causes.

#### **Partie 2 : Problématique – L'Aiguille dans la Botter de Foin Numérique**

Le constat est alarmant mais vague : les rendements globaux sont en baisse. Les hypothèses sont multiples :

- Certaines turbines ont-elles des pales dégradées ?
- Y a-t-il des arrêts non planifiés non remontés ?
- La maintenance est-elle mal calibrée ?

La question précise devient : **"Comment identifier, parmi 50 turbines et des millions de points de données, celles qui produisent moins de 60% de leur potentiel théorique (calculé à partir du vent réel) ?"**

L’analyse manuelle est impossible. Les outils classiques (Excel) échouent sur le volume. Vous avez besoin d’une approche industrialisée, capable de **nettoyer, croiser, enrichir et analyser** ces données massives pour en extraire une réponse claire et actionnable.

#### **Partie 3 : La Solution – Du Chaos des Données à la Décision Clairvoyante**

Grâce à l’architecture **Medallion (Bronze/Silver/Gold)** et à la puissance combinée de **Spark SQL et PySpark** dans Microsoft Fabric, nous avons transformé ce chaos en insights stratégiques en quelques heures.

1. **Notebook 1 (Bronze – Import)** : Avec **Spark SQL**, nous avons ingéré et stocké de manière fiable toutes les données brutes dans des tables `Delta`. Premier diagnostic : 5% de valeurs aberrantes, 3% de données manquantes.
2. **Notebook 2 (Silver – Nettoyage)** : **Spark SQL** a brillé pour le nettoyage basique : suppression des doublons, filtrage des valeurs nulles, standardisation des dates et premières jointures (production + vent). **Limite rencontrée** : Impossible d’appliquer la **courbe de puissance théorique** (logique métier complexe) en SQL pur.
3. **Notebook 3 (Silver – Calculs Avancés)** : **PySpark a débloqué la situation**. Nous avons créé une **UDF (User-Defined Function)** pour calculer, pour chaque mesure de vent, la **production théorique attendue**. Le calcul du **ratio de performance** (`production réelle / production théorique`) a alors révélé 8 turbines problématiques (ratio < 60%). Nous avons enrichi les données avec des indicateurs avancés (z-score pour les anomalies).
4. **Notebook 4 (Silver – ML)** : Avec **PySpark MLlib**, nous avons entraîné un modèle de prédiction. Il a non seulement confirmé les déficits de production, mais a aussi détecté des **dérives subtiles** non captées par les règles simples.
5. **Notebook 5 (Gold – Agrégations Métier)** : **PySpark** a agrégé les données au niveau quotidien et turbine pour créer les **KPIs business** : production perdue, temps d’arrêt équivalent, coût financier. Ces tables sont prêtes pour les rapports.
6. **Notebook 6 (Gold – Visualisation)** : Le tableau de bord final a **mis en lumière les 8 turbines défaillantes**, quantifié l’impact (5% de production perdue pour le parc) et a priorisé les actions de maintenance.

**Conclusion & Règle d'Or Démontrée :**
Cette analyse a permis d’identifier et de corriger des défauts (pales fissurées, capteurs déréglés), **récupérant 5% de la production du parc**. Elle a surtout validé une règle d’or pragmatique :

- **Spark SQL** est parfait pour **l'ingestion, l'exploration et le nettoyage initial** (rapide, déclaratif).
- **PySpark** devient **indispensable dès que la logique métier se complexifie** (UDF, fenêtrages avancés, feature engineering, Machine Learning), offrant une puissance et une flexibilité inégalées.

Vous passez ainsi d’une intuition floue à une décision data-driven, en démontrant la valeur concrète d’une plateforme data moderne.
